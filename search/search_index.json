{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Open Food Facts - Product Opener (Web Server) # Tests # What is Product Opener? **Product Opener** is the server software for **Open Food Facts** and **Open Beauty Facts**. It is released under the AGPL license and is being developed in Perl, HTML and JavaScript as [Free and Open-Source Software](https://en.wikipedia.org/wiki/Free_and_open-source_software). It works together with [Robotoff](https://github.com/openfoodfacts/robotoff), Open Food Facts' AI system (in Python, which can also be installed locally) and the [Open Food Facts apps](https://github.com/openfoodfacts/smooth-app) (which can work with your local instance after enabling dev mode) What is Open Food Facts? ### A food product database Open Food Facts is a database of food products with ingredients, allergens, nutritional facts and all the tidbits of information that is available on various product labels. ### Made by everyone Open Food Facts is a non-profit association of volunteers. 25.000+ contributors like you have added 1.7 million + products from 150 countries using our Android, iPhone or Windows Phone app or their camera to scan barcodes and upload pictures of products and their labels. ### For everyone Data about food is of public interest and has to be open (i.e available to everyone). The complete database is published as open data and can be reused by anyone and for any use. Check-out the cool reuses or make your own! https://world.openfoodfacts.org Weekly meetings * We e-meet Mondays at 16:00 Paris Time (15:00 London Time, 20:30 IST, 07:00 AM PT) * ![Google Meet](https://img.shields.io/badge/Google%20Meet-00897B?logo=google-meet&logoColor=white) Video call link: https://meet.google.com/nnw-qswu-hza * Join by phone: https://tel.meet/nnw-qswu-hza?pin=2111028061202 * Add the Event to your Calendar by [adding the Open Food Facts community calendar to your calendar](https://wiki.openfoodfacts.org/Events) * [Weekly Agenda](https://drive.google.com/open?id=1LL8-aiSF482xaJ1o0AKmhXB5QWfVE0_jzvYakq3VXys): please add the Agenda items as early as you can. * Make sure to check the Agenda items in advance of the meeting, so that we have the most informed discussions possible. * The meeting will handle Agenda items first, and if time permits, collaborative bug triage. * We strive to timebox the core of the meeting (decision making) to 30 minutes, with an optional free discussion/live debugging afterwards. * We take comprehensive notes in the Weekly Agenda of agenda item discussions and of decisions taken. Feature Sprint # We use feature-based sprints, tracked here User interface # Mockups on the current design and future plans to discuss Priorities # Top issues P1 problems P1 candidates Please add roadmaps here How do I get started? * Join us on Slack at in the channels: `#api`, `#productopener`, `#dev`. * [API Documentation](https://openfoodfacts.github.io/api-documentation/) * [NEW API Documentation (WIP)](https://openfoodfacts.github.io/openfoodfacts-server/reference/api/) ([source](https://github.com/openfoodfacts/openfoodfacts-server/tree/main/docs/reference/api.yml)) * Developer documentation: * [Server Documentation](https://openfoodfacts.github.io/openfoodfacts-server/) * [Quick start guide (Docker)](./introduction/dev-environment-quick-start-guide.md) * [Developer guide (Docker)](./how-to-guides/docker-developer-guide.md) * [Developer guide (Gitpod)](./how-to-guides/use-gitpod.md) * Configuration [TBA] * Dependencies [TBA] * Database configuration [TBA] * How to run tests [TBA] * [Perl modules documentation (POD)](https://openfoodfacts.github.io/reference/perl/) Note: documentation follows the [Di\u00e1taxis Framework](https://diataxis.fr/) Contribution guidelines # If you're new to Open-Source, we recommend you to check out our Contributing Guidelines . Feel free to fork the project and send us a pull request. Writing tests Code review Other guidelines Please add new features to the CHANGELOG.md file before or after merge to make testing easier Reporting problems or asking for a feature # Have a bug or a feature request? Please search for existing and closed issues. If your problem or idea is not addressed yet, please open a new issue . You can ask directly in the discussion room if you're not sure Translate Open Food Facts in your language # You can help translate the Open Food Facts web version and the app at : https://translate.openfoodfacts.org/ (no technical knowledge required, takes a minute to signup) Helping with HTML and CSS # We have templatized Product Opener, we use Gulp and NPM, but you'll need to run the Product Opener docker to be able to see the result (see the How do I get set up? section). In particular, you can help with issues on the new design . Who do I talk to? # Join our discussion room at https://slack.openfoodfacts.org/ Make sure to join the #productopener and #productopener-alerts channels. St\u00e9phane, Pierre, Charles or Hangy will be around to help you get started. Contributors # This project exists thanks to all the people who contribute. Backers # Thank you to all our backers! \ud83d\ude4f [ Become a backer ] Sponsors # Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [ Become a sponsor ] Open Food Facts Personal Search project was funded through the NGI0 Discovery Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme.","title":"Home"},{"location":"#open-food-facts-product-opener-web-server","text":"","title":"Open Food Facts - Product Opener (Web Server)"},{"location":"#tests","text":"","title":"Tests"},{"location":"#feature-sprint","text":"We use feature-based sprints, tracked here","title":"Feature Sprint"},{"location":"#user-interface","text":"Mockups on the current design and future plans to discuss","title":"User interface"},{"location":"#priorities","text":"Top issues P1 problems P1 candidates Please add roadmaps here","title":"Priorities"},{"location":"#contribution-guidelines","text":"If you're new to Open-Source, we recommend you to check out our Contributing Guidelines . Feel free to fork the project and send us a pull request. Writing tests Code review Other guidelines Please add new features to the CHANGELOG.md file before or after merge to make testing easier","title":"Contribution guidelines"},{"location":"#reporting-problems-or-asking-for-a-feature","text":"Have a bug or a feature request? Please search for existing and closed issues. If your problem or idea is not addressed yet, please open a new issue . You can ask directly in the discussion room if you're not sure","title":"Reporting problems or asking for a feature"},{"location":"#translate-open-food-facts-in-your-language","text":"You can help translate the Open Food Facts web version and the app at : https://translate.openfoodfacts.org/ (no technical knowledge required, takes a minute to signup)","title":"Translate Open Food Facts in your language"},{"location":"#helping-with-html-and-css","text":"We have templatized Product Opener, we use Gulp and NPM, but you'll need to run the Product Opener docker to be able to see the result (see the How do I get set up? section). In particular, you can help with issues on the new design .","title":"Helping with HTML and CSS"},{"location":"#who-do-i-talk-to","text":"Join our discussion room at https://slack.openfoodfacts.org/ Make sure to join the #productopener and #productopener-alerts channels. St\u00e9phane, Pierre, Charles or Hangy will be around to help you get started.","title":"Who do I talk to?"},{"location":"#contributors","text":"This project exists thanks to all the people who contribute.","title":"Contributors"},{"location":"#backers","text":"Thank you to all our backers! \ud83d\ude4f [ Become a backer ]","title":"Backers"},{"location":"#sponsors","text":"Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [ Become a sponsor ] Open Food Facts Personal Search project was funded through the NGI0 Discovery Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme.","title":"Sponsors"},{"location":"explanations/packaging-data/","text":"Packaging data # This document explains how packaging data is currently added, updated and structured in the Open Food Facts database, and how it could be improved. Introduction # Types of packaging data # Food products typically have 1 or more packaging components (e.g. milk may have a bottle and a cap). For each product, we aim to have a comprehensive list of all its packaging components, with detailed information about each packaging component. Data about packaging components # For each packaging component, we want data for different attributes, like its shape (e.g. a bottle) and its size (e.g. plastic). There are many different attributes that can be interesting for specific uses. For instance, researchers in epidemiology are interested in knowing which packaging component is in contact with the food itself, and which one can be put in the microwave oven, so that they can study the long term effects of some plastics on health. Sources of packaging data # We can get packaging data from different sources: Users # Users of the Open Food Facts website and app, and users of 3rd party apps, can enter packaging data. Manufacturers # Some manufacturers send product data through GS1, which currently has limited support for packaging information (but this is likely to be improved in the years to come). Some manufacturers send us more detailed packaging data (e.g. recycling instructions) through the Producers Platform. Some manufacturers send us data used to compute the Eco-Score using the Eco-Score spreadsheet template, which has fields like \"Packaging 1\", \"Material 1\", \"Packaging 2\", \"Material 2\" etc. Product photos and machine learning # We can extract logos related to packaging, or parse the text recognized from product photos to recognize packaging information or recycling instructions. How packaging data is currently added, updated and structured in Open Food Facts # In Open Food Facts, we currently have a number of input fields related to packaging. The data in those fields is parsed and analyzed to create a structured list of packaging components with specific attributes. Current input fields # Packaging tag field (READ and WRITE) # At the start of Open Food Facts in 2012, we had a \"packaging\" tag field where users could enter comma separated free text entries about the packaging (e.g. \"Plastic\", \"Bag\" or \"Plastic bag\") in different languages. In 2020, we made this field a taxonomized field. As a result, we now store the language used to fill this field, so that we can match its value to the multilingual packaging taxonomy. So \"plastique\" in French will be mapped to the canonical \"en:plastic\" entry. Packaging information / recycling instructions text field (READ and WRITE) # In 2020, we also added a language specific field (\"packaging_text_[language code]\" e.g. \"packaging_text_en\" for English) to store free text data about the packaging. It can contain the text of the recycling instructions printed on the packaging (e.g. \"bottle to recycle, cap to discard\"), or can be filled in by users (e.g. \"1 PET plastic bottle to recycle, 1 plastic cap\"). Current resulting packagings data structure (READ only) # The input fields are analyzed and combined to create the \"packagings\" data structure. The structure is an array of packaging components. Each packaging component can have values for different attributes: number: the number of units for the packaging component (e.g. a pack of beers may contain 6 bottles) shape: the general shape of the packaging component (e.g. \"bottle\", \"box\") material: the material of the packaging component quantity: how much product the packaging component contains (e.g. \"25 cl\") recycling: whether the packaging component should be recycled, discarded or reused The \"shape\" and \"material\" fields are taxonomized using the packaging_shapes and packaging_materials taxonomies. How the the resulting packagings data structure is created # Extract attributes that relate to different packaging components # The values for each input field (\"packaging\" tag field and \"packaging_text_[language code]\" packaging information text field) are analyzed 1 to recognize packaging components and their attributes. One product may have multiple \"packaging_text_[language code]\" values in different languages. Only the value for the main product of the language is currently analyzed. For instance, if the \"packaging\" field contains \"Plastic bottle, box, cardboard\", we will use the packaging shapes, materials and recycling taxonomies to create a list of 3 packaging components: {shape:\"en:bottle\", material:\"en:plastic\"}, {shape:\"en:box\"}, {material:\"en:cardboard\"}. And if the \"packaging_text_en\" field contains \"PET bottle to recycle, box to reuse\", we will create 2 more packaging components: {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}, {shape:\"box\", recycling:\"reuse\"}. Merge packaging components # The 3 + 2 = 5 resulting packaging components are then added one by one in the packagings structure. When their attributes are compatible, the packaging units are merged 2 . For instance {shape:\"en:box\"} and {material:\"en:cardboard\"} have non conflicting attributes, so they are merged into {shape:\"en:box\", material:\"en:cardboard\"}. Note that it is possible that this is a mistake, and that the \"box\" and \"cardboard\" tags concern in fact different components. Similarly, as \"en:plastic\" is a parent of \"en:pet-polyethylene-terephthalate\" in the packaging_materials taxonomy, we can merge {shape:\"en:bottle\", material:\"en:plastic\"} with {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"} into {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}. The resulting structure is: packagings: [ { material: \"en:pet-polyethylene-terephthalate\", recycling: \"en:recycle\", shape: \"en:bottle\" }, { recycling: \"en:reuse\", shape: \"en:box\" }, { shape: \"en:container\" } ] Taxonomies # We have created a number of multilingual taxonomies related to packagings: Packaging shapes taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_shapes.txt Packaging materials taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_materials.txt Packaging recycling taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_recycling.txt Preservation methods taxonomy (related) : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/preservation.txt Those taxonomies are used to structure packaging data in Open Food Facts, and to analyze unstructured input data. How we could improve it # Extend the attributes of the packaging components in the \"packagings\" data structure # Weight # We need to add an attribute for the weight of the packaging component. We might need to add different fields to distinguish values that have been entered by users that weight the packaging, versus values provided by the manufacturer, or average values that we have determined from other products, or that we got from external sources. Make the \"packagings\" data structure READ and WRITE # The \"packagings\" data structure is currently a READ only field. We could create an API to make it a READ and WRITE field. For new products, clients (website and apps) could ask users to enter data about all packaging components of the product. For existing products, clients could display the packaging components and let users change them (e.g. adding or removing components, entering values for new attributes, editing attributes to add more precise values (e.g. which type of plastic) etc.). Add a way to indicate that the \"packagings\" data structure contains all the packaging components of the product # We currently have no way to know if the packaging data we have for a product is complete, or if we may be missing some packaging components. We could have a way (e.g. a checkbox) that users could use to indicate all components are accounted for. And we could also do the reverse, and indicate that it is very likely that we are missing some packaging components (e.g. if we have a \"cap\" but no other component to put the cap on). Deprecate the \"packaging\" tags field # We could discard the existing \"packaging\" tags field, and replace it with an API to allow clients to add partial information about packaging components. For instance, if Robotoff detects that the product is in plastic bottle by analyzing a product photo, it could send {shape:\"bottle\", material:\"en:plastic\"} and it would be added / combined with the existing \"packagings\" data. Keep the \"packaging_text_[language code]\" field # It is important to keep this field, as we can display it as-is, use it as input data, and it may contain interesting data that we do not analyze yet. When filled, the values for this field can be analyzed and added to / combined with the \"packagings\" data structure. Similarly to ingredient text analysis, we could keep information about which parts of the text were recognized as attributes of a packaging component, and which parts were not recognized and were therefore ignored. Changing the \"packagings\" value will not change the \"packaging_text_[language code]\" values. Challenges # Incomplete lists of packaging components # Slightly mismatched data from different sources # For a single product, we might get partial packaging data from different sources that we map to similar but distinct shapes, like \"bottle\", \"jar\" and \"jug\". It may be difficult to determine if the data concerns a single packaging component, or different components. Products with packaging changes # Ressources # 2020 project to start structuring packaging data: https://wiki.openfoodfacts.org/Packagings_data_structure parse_packaging_from_text_phrase() function in /lib/ProductOpener/Packagings.pm \u21a9 analyze_and_combine_packaging_data() function in /lib/ProductOpener/Packagings.pm \u21a9","title":"Packaging data"},{"location":"explanations/packaging-data/#packaging-data","text":"This document explains how packaging data is currently added, updated and structured in the Open Food Facts database, and how it could be improved.","title":"Packaging data"},{"location":"explanations/packaging-data/#introduction","text":"","title":"Introduction"},{"location":"explanations/packaging-data/#types-of-packaging-data","text":"Food products typically have 1 or more packaging components (e.g. milk may have a bottle and a cap). For each product, we aim to have a comprehensive list of all its packaging components, with detailed information about each packaging component.","title":"Types of packaging data"},{"location":"explanations/packaging-data/#data-about-packaging-components","text":"For each packaging component, we want data for different attributes, like its shape (e.g. a bottle) and its size (e.g. plastic). There are many different attributes that can be interesting for specific uses. For instance, researchers in epidemiology are interested in knowing which packaging component is in contact with the food itself, and which one can be put in the microwave oven, so that they can study the long term effects of some plastics on health.","title":"Data about packaging components"},{"location":"explanations/packaging-data/#sources-of-packaging-data","text":"We can get packaging data from different sources:","title":"Sources of packaging data"},{"location":"explanations/packaging-data/#users","text":"Users of the Open Food Facts website and app, and users of 3rd party apps, can enter packaging data.","title":"Users"},{"location":"explanations/packaging-data/#manufacturers","text":"Some manufacturers send product data through GS1, which currently has limited support for packaging information (but this is likely to be improved in the years to come). Some manufacturers send us more detailed packaging data (e.g. recycling instructions) through the Producers Platform. Some manufacturers send us data used to compute the Eco-Score using the Eco-Score spreadsheet template, which has fields like \"Packaging 1\", \"Material 1\", \"Packaging 2\", \"Material 2\" etc.","title":"Manufacturers"},{"location":"explanations/packaging-data/#product-photos-and-machine-learning","text":"We can extract logos related to packaging, or parse the text recognized from product photos to recognize packaging information or recycling instructions.","title":"Product photos and machine learning"},{"location":"explanations/packaging-data/#how-packaging-data-is-currently-added-updated-and-structured-in-open-food-facts","text":"In Open Food Facts, we currently have a number of input fields related to packaging. The data in those fields is parsed and analyzed to create a structured list of packaging components with specific attributes.","title":"How packaging data is currently added, updated and structured in Open Food Facts"},{"location":"explanations/packaging-data/#current-input-fields","text":"","title":"Current input fields"},{"location":"explanations/packaging-data/#packaging-tag-field-read-and-write","text":"At the start of Open Food Facts in 2012, we had a \"packaging\" tag field where users could enter comma separated free text entries about the packaging (e.g. \"Plastic\", \"Bag\" or \"Plastic bag\") in different languages. In 2020, we made this field a taxonomized field. As a result, we now store the language used to fill this field, so that we can match its value to the multilingual packaging taxonomy. So \"plastique\" in French will be mapped to the canonical \"en:plastic\" entry.","title":"Packaging tag field (READ and WRITE)"},{"location":"explanations/packaging-data/#packaging-information-recycling-instructions-text-field-read-and-write","text":"In 2020, we also added a language specific field (\"packaging_text_[language code]\" e.g. \"packaging_text_en\" for English) to store free text data about the packaging. It can contain the text of the recycling instructions printed on the packaging (e.g. \"bottle to recycle, cap to discard\"), or can be filled in by users (e.g. \"1 PET plastic bottle to recycle, 1 plastic cap\").","title":"Packaging information / recycling instructions text field (READ and WRITE)"},{"location":"explanations/packaging-data/#current-resulting-packagings-data-structure-read-only","text":"The input fields are analyzed and combined to create the \"packagings\" data structure. The structure is an array of packaging components. Each packaging component can have values for different attributes: number: the number of units for the packaging component (e.g. a pack of beers may contain 6 bottles) shape: the general shape of the packaging component (e.g. \"bottle\", \"box\") material: the material of the packaging component quantity: how much product the packaging component contains (e.g. \"25 cl\") recycling: whether the packaging component should be recycled, discarded or reused The \"shape\" and \"material\" fields are taxonomized using the packaging_shapes and packaging_materials taxonomies.","title":"Current resulting packagings data structure (READ only)"},{"location":"explanations/packaging-data/#how-the-the-resulting-packagings-data-structure-is-created","text":"","title":"How the the resulting packagings data structure is created"},{"location":"explanations/packaging-data/#extract-attributes-that-relate-to-different-packaging-components","text":"The values for each input field (\"packaging\" tag field and \"packaging_text_[language code]\" packaging information text field) are analyzed 1 to recognize packaging components and their attributes. One product may have multiple \"packaging_text_[language code]\" values in different languages. Only the value for the main product of the language is currently analyzed. For instance, if the \"packaging\" field contains \"Plastic bottle, box, cardboard\", we will use the packaging shapes, materials and recycling taxonomies to create a list of 3 packaging components: {shape:\"en:bottle\", material:\"en:plastic\"}, {shape:\"en:box\"}, {material:\"en:cardboard\"}. And if the \"packaging_text_en\" field contains \"PET bottle to recycle, box to reuse\", we will create 2 more packaging components: {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}, {shape:\"box\", recycling:\"reuse\"}.","title":"Extract attributes that relate to different packaging components"},{"location":"explanations/packaging-data/#merge-packaging-components","text":"The 3 + 2 = 5 resulting packaging components are then added one by one in the packagings structure. When their attributes are compatible, the packaging units are merged 2 . For instance {shape:\"en:box\"} and {material:\"en:cardboard\"} have non conflicting attributes, so they are merged into {shape:\"en:box\", material:\"en:cardboard\"}. Note that it is possible that this is a mistake, and that the \"box\" and \"cardboard\" tags concern in fact different components. Similarly, as \"en:plastic\" is a parent of \"en:pet-polyethylene-terephthalate\" in the packaging_materials taxonomy, we can merge {shape:\"en:bottle\", material:\"en:plastic\"} with {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"} into {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}. The resulting structure is: packagings: [ { material: \"en:pet-polyethylene-terephthalate\", recycling: \"en:recycle\", shape: \"en:bottle\" }, { recycling: \"en:reuse\", shape: \"en:box\" }, { shape: \"en:container\" } ]","title":"Merge packaging components"},{"location":"explanations/packaging-data/#taxonomies","text":"We have created a number of multilingual taxonomies related to packagings: Packaging shapes taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_shapes.txt Packaging materials taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_materials.txt Packaging recycling taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_recycling.txt Preservation methods taxonomy (related) : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/preservation.txt Those taxonomies are used to structure packaging data in Open Food Facts, and to analyze unstructured input data.","title":"Taxonomies"},{"location":"explanations/packaging-data/#how-we-could-improve-it","text":"","title":"How we could improve it"},{"location":"explanations/packaging-data/#extend-the-attributes-of-the-packaging-components-in-the-packagings-data-structure","text":"","title":"Extend the attributes of the packaging components in the \"packagings\" data structure"},{"location":"explanations/packaging-data/#weight","text":"We need to add an attribute for the weight of the packaging component. We might need to add different fields to distinguish values that have been entered by users that weight the packaging, versus values provided by the manufacturer, or average values that we have determined from other products, or that we got from external sources.","title":"Weight"},{"location":"explanations/packaging-data/#make-the-packagings-data-structure-read-and-write","text":"The \"packagings\" data structure is currently a READ only field. We could create an API to make it a READ and WRITE field. For new products, clients (website and apps) could ask users to enter data about all packaging components of the product. For existing products, clients could display the packaging components and let users change them (e.g. adding or removing components, entering values for new attributes, editing attributes to add more precise values (e.g. which type of plastic) etc.).","title":"Make the \"packagings\" data structure READ and WRITE"},{"location":"explanations/packaging-data/#add-a-way-to-indicate-that-the-packagings-data-structure-contains-all-the-packaging-components-of-the-product","text":"We currently have no way to know if the packaging data we have for a product is complete, or if we may be missing some packaging components. We could have a way (e.g. a checkbox) that users could use to indicate all components are accounted for. And we could also do the reverse, and indicate that it is very likely that we are missing some packaging components (e.g. if we have a \"cap\" but no other component to put the cap on).","title":"Add a way to indicate that the \"packagings\" data structure contains all the packaging components of the product"},{"location":"explanations/packaging-data/#deprecate-the-packaging-tags-field","text":"We could discard the existing \"packaging\" tags field, and replace it with an API to allow clients to add partial information about packaging components. For instance, if Robotoff detects that the product is in plastic bottle by analyzing a product photo, it could send {shape:\"bottle\", material:\"en:plastic\"} and it would be added / combined with the existing \"packagings\" data.","title":"Deprecate the \"packaging\" tags field"},{"location":"explanations/packaging-data/#keep-the-packaging_text_language-code-field","text":"It is important to keep this field, as we can display it as-is, use it as input data, and it may contain interesting data that we do not analyze yet. When filled, the values for this field can be analyzed and added to / combined with the \"packagings\" data structure. Similarly to ingredient text analysis, we could keep information about which parts of the text were recognized as attributes of a packaging component, and which parts were not recognized and were therefore ignored. Changing the \"packagings\" value will not change the \"packaging_text_[language code]\" values.","title":"Keep the \"packaging_text_[language code]\" field"},{"location":"explanations/packaging-data/#challenges","text":"","title":"Challenges"},{"location":"explanations/packaging-data/#incomplete-lists-of-packaging-components","text":"","title":"Incomplete lists of packaging components"},{"location":"explanations/packaging-data/#slightly-mismatched-data-from-different-sources","text":"For a single product, we might get partial packaging data from different sources that we map to similar but distinct shapes, like \"bottle\", \"jar\" and \"jug\". It may be difficult to determine if the data concerns a single packaging component, or different components.","title":"Slightly mismatched data from different sources"},{"location":"explanations/packaging-data/#products-with-packaging-changes","text":"","title":"Products with packaging changes"},{"location":"explanations/packaging-data/#ressources","text":"2020 project to start structuring packaging data: https://wiki.openfoodfacts.org/Packagings_data_structure parse_packaging_from_text_phrase() function in /lib/ProductOpener/Packagings.pm \u21a9 analyze_and_combine_packaging_data() function in /lib/ProductOpener/Packagings.pm \u21a9","title":"Ressources"},{"location":"explanations/pro-dev-setup/","text":"Docker Setup of pro platform for development # This explains how we setup docker file for pro platform development. For explanations on how to use it, see: how-to-guides/pro-development off is the public facing application (world.openfoodfacts.org) off-pro is the producers platform (world.pro.openfoodfacts.org) When we work on the pro platform for development we want: * off containers to talk between each other, and have their own volumes * off-pro containers to talk between each other, and, generally, have their own volumes * minion and backend from both app access to the same postgres database (which stores tasks queues) * off and off-pro backends / minion needs to share some volumes : orgs, users ands some files living in podata Still we would like to avoid having different clone of the repository, but we can isolate projects thanks to COMPOSE_PROJECT_NAME , which will prefix containers names, volumes and default network, thus isolate each projects. This is achieved by sourcing the .env-pro which setup some environment variables that will superseed the .env variables. The main one being setting COMPOSE_PROJECT_NAME and PRODUCERS_PLATFORM , but also other like MINION_QUEUE . On volume side, we will simply give hard-coded names to volumes that should be shared between off and pro platform, thus they will be shared. Ideally we should not have to share single files but this is a work in progress, we will live without it as a first approx. To satisfy the access to the same database, we will use postgres database from off as the common database. In order to achieve that: * we use profiles, so we won't start postgres in pro docker-compose * we connect postgres , backend and minion services to a shared network, called minion_db Fortunately this works, but note that there is a pitfall: on minion_db network both backend services ( off and off-pro ) will respond to same name. For the moment it is not a problem for we don't need to communicate directly between instances. If it was, we would have to define custom aliases for those services on the minion_db network. network OFF network PRO network po_default containers minion_db containers po_pro_default | | | +------postgres------------+ | | | | | | | +-----backend--------------+ | | +----------backend-------------+ | | | +------minion--------------+ | | +----------minion--------------+ | | | | | | +------frontend | frontend------------+ +------mongodb | mongodb-------------+ | | |","title":"Docker Setup of pro platform for development"},{"location":"explanations/pro-dev-setup/#docker-setup-of-pro-platform-for-development","text":"This explains how we setup docker file for pro platform development. For explanations on how to use it, see: how-to-guides/pro-development off is the public facing application (world.openfoodfacts.org) off-pro is the producers platform (world.pro.openfoodfacts.org) When we work on the pro platform for development we want: * off containers to talk between each other, and have their own volumes * off-pro containers to talk between each other, and, generally, have their own volumes * minion and backend from both app access to the same postgres database (which stores tasks queues) * off and off-pro backends / minion needs to share some volumes : orgs, users ands some files living in podata Still we would like to avoid having different clone of the repository, but we can isolate projects thanks to COMPOSE_PROJECT_NAME , which will prefix containers names, volumes and default network, thus isolate each projects. This is achieved by sourcing the .env-pro which setup some environment variables that will superseed the .env variables. The main one being setting COMPOSE_PROJECT_NAME and PRODUCERS_PLATFORM , but also other like MINION_QUEUE . On volume side, we will simply give hard-coded names to volumes that should be shared between off and pro platform, thus they will be shared. Ideally we should not have to share single files but this is a work in progress, we will live without it as a first approx. To satisfy the access to the same database, we will use postgres database from off as the common database. In order to achieve that: * we use profiles, so we won't start postgres in pro docker-compose * we connect postgres , backend and minion services to a shared network, called minion_db Fortunately this works, but note that there is a pitfall: on minion_db network both backend services ( off and off-pro ) will respond to same name. For the moment it is not a problem for we don't need to communicate directly between instances. If it was, we would have to define custom aliases for those services on the minion_db network. network OFF network PRO network po_default containers minion_db containers po_pro_default | | | +------postgres------------+ | | | | | | | +-----backend--------------+ | | +----------backend-------------+ | | | +------minion--------------+ | | +----------minion--------------+ | | | | | | +------frontend | frontend------------+ +------mongodb | mongodb-------------+ | | |","title":"Docker Setup of pro platform for development"},{"location":"how-to-guides/deploy/","text":"Prod environment deployment guide # Note: prod deployment is very manual and not automated yet. Login to the off1 server, as the \"off\" user cd /home/off/openfoodfacts-server Check that you are on the main branch git pull Copy changed files (don't copy everything, in particular not the lang directory that is being moved to the openfoodfacts-web repository) e.g. cp cgi scripts lib po taxonomies templates /srv/off/ cd /srv/off export NPM_CONFIG_PREFIX=~/.npm-global npm install npm run build cd /srv/off/cgi export PERL5LIB=. ./build_lang.pl as the root user: systemctl stop apache2@off systemctl start apache2@off systemctl stop minion-off systemctl start minion-off","title":"Prod environment deployment guide"},{"location":"how-to-guides/deploy/#prod-environment-deployment-guide","text":"Note: prod deployment is very manual and not automated yet. Login to the off1 server, as the \"off\" user cd /home/off/openfoodfacts-server Check that you are on the main branch git pull Copy changed files (don't copy everything, in particular not the lang directory that is being moved to the openfoodfacts-web repository) e.g. cp cgi scripts lib po taxonomies templates /srv/off/ cd /srv/off export NPM_CONFIG_PREFIX=~/.npm-global npm install npm run build cd /srv/off/cgi export PERL5LIB=. ./build_lang.pl as the root user: systemctl stop apache2@off systemctl start apache2@off systemctl stop minion-off systemctl start minion-off","title":"Prod environment deployment guide"},{"location":"how-to-guides/docker-developer-guide/","text":"Docker Developer's Guide # This guide is for developers and newcomers to help them debug and explore Docker. This page describes how to test and debug your changes once you have set up the project, Product Opener with Docker using dev environment quick start guide . Checking logs # Tail Docker Compose logs # make log You will get logs from nginx, mongodb, postgres, etc. Tail other logs # Most logs from perl are not (yet ?) displayed on the docker logs, but are instead available in specific directories. To see them use: make tail It will tail -f all the files present in the logs/ directory: apache2/error.log apache2/log4perl.log apache2/modperl_error.log apache2/other_vhosts_access.log nginx/access.log nginx/error.log You can also simply run: tail -f <FILEPATH> to check a specific log. One of the most important is log4perl.log . Increasing log verbosity # By default, the log4perl configuration conf/log.conf matches production settings. You can tweak that file with your own dev configuration settings and run make restart to reload the changes. A setting useful for local environments is to set TRACE log level: log4perl.rootLogger=TRACE, LOGFILE Opening a shell in a Docker container # Run the following to open a bash shell within the backend container: docker-compose exec backend bash You should see root@<CONTAINER_ID>:/# (opened root shell): you are now within the Docker container and can begin typing some commands ! Checking permissions # Navigate to the directory the specific directory and run ls -lrt It will list all the directories and their permission status. Creating directory # Navigate to your specific directory using cd command and run mkdir directory-name Running minion jobs # Minion is a high-performance job queue for Perl. Minion is used in openfoodfacts-server for time-consuming import and export tasks. These tasks are processed and queued using the minion jobs queue. Therefore, they are called minion jobs. Go to /opt/product-opener/scripts and run ./minion_producers.pl minion job The above command will show the status of minion jobs. Run the following command to launch the minion jobs. ./minion_producers.pl minion worker -m production -q pro.openfoodfacts.org Restarting Apache # Sometimes restarting the whole backend container is overkill, and you can just restart Apache from inside the container: apache2ctl -k restart Exiting the container # Use exit to exit the container. Making code changes # In the dev environment, any code change to the local directory will be written on the container. That said, some code changes require a restart of the backend container, or rebuilding the NPM assets. Getting away from make up # make up is a good command for starters, but it's not the right one to use if you develop on a daily bases, because it maybe slow, as it does a full rebuild, which, in dev mode, should only be necessary in a few cases. On a daily bases you could better run those: docker-compose up to start and monitor the stack. docker-compose restart backend to account for a code change in a .pm file (cgi pl files do not need a restart) docker-compose stop to stop them all If some important file changed (like Dockerfile or cpanfile, etc.), or in case of doubt, you can run docker-compose build (or maybe it's a good time to use make up once) You should explore the docker-compose commands most are useful! Live reload # To automate the live reload on code changes, you can install the Python package when-changed : pip3 install when-changed when-changed -r docker/ docker-compose.yml .env -c \"make restart\" # restart backend container on compose changes when-changed -r lib/ -r docker/ docker-compose.yml -c \"docker-compose backend restart\" # restart Apache on code changes when-changed -r html/ Dockerfile Dockerfile.frontend package.json -c \"make up\" # rebuild containers on asset or Dockerfile changes An alternative to when-changed is inotifywait . Run queries on MongoDB database # docker-compose exec mongodb mongo The above command will open a MongoDB shell, allowing you to use all the mongo commands to interact with the database: show dbs use off db.products.count() db.products.find({_id: \"5053990155354\"}) db.products.deleteOne({_id: \"5053990155354\"}) See the mongo shell docs for more commands. Adding environment variables # If you need some value to be configurable, it is best to set is as an environment variable. To add a new environment variable TEST : In .env file, add TEST=test_val [local]. In .github/workflows/container-deploy.yml , add echo \"TEST=${{ secrets.TEST }}\" >> .env to the \"Set environment variables\" build step [remote]. Also add the corresponding GitHub secret TEST=test_val . In docker-compose.yml file, add it under the backend > environment section. In conf/apache.conf file, add PerlPassEnv TEST . In lib/Config2.pm , add $test = $ENV{TEST}; . Also add $test to the EXPORT_OK list at the top of the file to avoid a compilation error. The call stack goes like this: make up > docker-compose > loads .env > pass env variables to the backend container > pass to mod_perl > initialized in Config2.pm . Managing multiple deployments # To juggle between multiple local deployments (e.g: to run different flavors of Open Food Facts on the same host), there are different possible strategies. a set env script # Docker-compose takes it settings from, in order of priority: * the environment * the .env file So one strategy to have a different instance, can be to keep same .env file, but super-seed some env variables to tweak the configuration. This is a good strategy for the pro plateform. For this case we have a setenv-pro.sh script. To use it, open a terminal, where you want to be in pro environment and simply use: . setenv-pro.sh then you can use whatever docker-compose command. Note: This terminal will remain in pro mode until you end its session. See also Developing on the producers platform different .env file # This strategy might be the right one if your settings differs a lot. You will need: Multiple .env files (one per deployment), such as: .env.off : configuration for Open Food Facts dev env. .env.off-pro : configuration for Open Food Facts Producer's Platform dev env. .env.obf : configuration for Open Beauty Facts dev env. .env.opff : configuration for Open Ped Food Facts dev env. COMPOSE_PROJECT_NAME set to different values in each .env file, so that container names across deployments are unique. FRONTEND_PORT and MONGODB_PORT set to different values in each .env file, so that frontend containers don't port-conflict with each other. To switch between configurations, set ENV_FILE before running make commands, (or docker-compose command): ENV_FILE=.env.off-pro make up # starts the OFF Producer's Platform containers. ENV_FILE=.env.obf make up # starts the OBF containers. ENV_FILE=.env.opff make up # starts the OPFF containers. or export it to keep it for a while: export ENV_FILE=.env.off # going to work on OFF for a while make up make restart make down make log A good strategy is to have multiple terminals open, one for each deployment: off [Terminal 1]: export ENV_FILE=.env.off make up off-pro [Terminal 2]: export ENV_FILE=.env.off-pro make up obf [Terminal 3]: export ENV_FILE=.env.obf make up opff [Terminal 3]: export ENV_FILE=.env.opff make up Note: the above case of 4 deployments is a bit ambitious , since ProductOpener's backend container takes about ~6GB of RAM to run, meaning that the above 4 deployments would require a total of 24GB of RAM available.","title":"Docker Developer's Guide"},{"location":"how-to-guides/docker-developer-guide/#docker-developers-guide","text":"This guide is for developers and newcomers to help them debug and explore Docker. This page describes how to test and debug your changes once you have set up the project, Product Opener with Docker using dev environment quick start guide .","title":"Docker Developer's Guide"},{"location":"how-to-guides/docker-developer-guide/#checking-logs","text":"","title":"Checking logs"},{"location":"how-to-guides/docker-developer-guide/#tail-docker-compose-logs","text":"make log You will get logs from nginx, mongodb, postgres, etc.","title":"Tail Docker Compose logs"},{"location":"how-to-guides/docker-developer-guide/#tail-other-logs","text":"Most logs from perl are not (yet ?) displayed on the docker logs, but are instead available in specific directories. To see them use: make tail It will tail -f all the files present in the logs/ directory: apache2/error.log apache2/log4perl.log apache2/modperl_error.log apache2/other_vhosts_access.log nginx/access.log nginx/error.log You can also simply run: tail -f <FILEPATH> to check a specific log. One of the most important is log4perl.log .","title":"Tail other logs"},{"location":"how-to-guides/docker-developer-guide/#increasing-log-verbosity","text":"By default, the log4perl configuration conf/log.conf matches production settings. You can tweak that file with your own dev configuration settings and run make restart to reload the changes. A setting useful for local environments is to set TRACE log level: log4perl.rootLogger=TRACE, LOGFILE","title":"Increasing log verbosity"},{"location":"how-to-guides/docker-developer-guide/#opening-a-shell-in-a-docker-container","text":"Run the following to open a bash shell within the backend container: docker-compose exec backend bash You should see root@<CONTAINER_ID>:/# (opened root shell): you are now within the Docker container and can begin typing some commands !","title":"Opening a shell in a Docker container"},{"location":"how-to-guides/docker-developer-guide/#checking-permissions","text":"Navigate to the directory the specific directory and run ls -lrt It will list all the directories and their permission status.","title":"Checking permissions"},{"location":"how-to-guides/docker-developer-guide/#creating-directory","text":"Navigate to your specific directory using cd command and run mkdir directory-name","title":"Creating directory"},{"location":"how-to-guides/docker-developer-guide/#running-minion-jobs","text":"Minion is a high-performance job queue for Perl. Minion is used in openfoodfacts-server for time-consuming import and export tasks. These tasks are processed and queued using the minion jobs queue. Therefore, they are called minion jobs. Go to /opt/product-opener/scripts and run ./minion_producers.pl minion job The above command will show the status of minion jobs. Run the following command to launch the minion jobs. ./minion_producers.pl minion worker -m production -q pro.openfoodfacts.org","title":"Running minion jobs"},{"location":"how-to-guides/docker-developer-guide/#restarting-apache","text":"Sometimes restarting the whole backend container is overkill, and you can just restart Apache from inside the container: apache2ctl -k restart","title":"Restarting Apache"},{"location":"how-to-guides/docker-developer-guide/#exiting-the-container","text":"Use exit to exit the container.","title":"Exiting the container"},{"location":"how-to-guides/docker-developer-guide/#making-code-changes","text":"In the dev environment, any code change to the local directory will be written on the container. That said, some code changes require a restart of the backend container, or rebuilding the NPM assets.","title":"Making code changes"},{"location":"how-to-guides/docker-developer-guide/#getting-away-from-make-up","text":"make up is a good command for starters, but it's not the right one to use if you develop on a daily bases, because it maybe slow, as it does a full rebuild, which, in dev mode, should only be necessary in a few cases. On a daily bases you could better run those: docker-compose up to start and monitor the stack. docker-compose restart backend to account for a code change in a .pm file (cgi pl files do not need a restart) docker-compose stop to stop them all If some important file changed (like Dockerfile or cpanfile, etc.), or in case of doubt, you can run docker-compose build (or maybe it's a good time to use make up once) You should explore the docker-compose commands most are useful!","title":"Getting away from make up"},{"location":"how-to-guides/docker-developer-guide/#live-reload","text":"To automate the live reload on code changes, you can install the Python package when-changed : pip3 install when-changed when-changed -r docker/ docker-compose.yml .env -c \"make restart\" # restart backend container on compose changes when-changed -r lib/ -r docker/ docker-compose.yml -c \"docker-compose backend restart\" # restart Apache on code changes when-changed -r html/ Dockerfile Dockerfile.frontend package.json -c \"make up\" # rebuild containers on asset or Dockerfile changes An alternative to when-changed is inotifywait .","title":"Live reload"},{"location":"how-to-guides/docker-developer-guide/#run-queries-on-mongodb-database","text":"docker-compose exec mongodb mongo The above command will open a MongoDB shell, allowing you to use all the mongo commands to interact with the database: show dbs use off db.products.count() db.products.find({_id: \"5053990155354\"}) db.products.deleteOne({_id: \"5053990155354\"}) See the mongo shell docs for more commands.","title":"Run queries on MongoDB database"},{"location":"how-to-guides/docker-developer-guide/#adding-environment-variables","text":"If you need some value to be configurable, it is best to set is as an environment variable. To add a new environment variable TEST : In .env file, add TEST=test_val [local]. In .github/workflows/container-deploy.yml , add echo \"TEST=${{ secrets.TEST }}\" >> .env to the \"Set environment variables\" build step [remote]. Also add the corresponding GitHub secret TEST=test_val . In docker-compose.yml file, add it under the backend > environment section. In conf/apache.conf file, add PerlPassEnv TEST . In lib/Config2.pm , add $test = $ENV{TEST}; . Also add $test to the EXPORT_OK list at the top of the file to avoid a compilation error. The call stack goes like this: make up > docker-compose > loads .env > pass env variables to the backend container > pass to mod_perl > initialized in Config2.pm .","title":"Adding environment variables"},{"location":"how-to-guides/docker-developer-guide/#managing-multiple-deployments","text":"To juggle between multiple local deployments (e.g: to run different flavors of Open Food Facts on the same host), there are different possible strategies.","title":"Managing multiple deployments"},{"location":"how-to-guides/docker-developer-guide/#a-set-env-script","text":"Docker-compose takes it settings from, in order of priority: * the environment * the .env file So one strategy to have a different instance, can be to keep same .env file, but super-seed some env variables to tweak the configuration. This is a good strategy for the pro plateform. For this case we have a setenv-pro.sh script. To use it, open a terminal, where you want to be in pro environment and simply use: . setenv-pro.sh then you can use whatever docker-compose command. Note: This terminal will remain in pro mode until you end its session. See also Developing on the producers platform","title":"a set env script"},{"location":"how-to-guides/docker-developer-guide/#different-env-file","text":"This strategy might be the right one if your settings differs a lot. You will need: Multiple .env files (one per deployment), such as: .env.off : configuration for Open Food Facts dev env. .env.off-pro : configuration for Open Food Facts Producer's Platform dev env. .env.obf : configuration for Open Beauty Facts dev env. .env.opff : configuration for Open Ped Food Facts dev env. COMPOSE_PROJECT_NAME set to different values in each .env file, so that container names across deployments are unique. FRONTEND_PORT and MONGODB_PORT set to different values in each .env file, so that frontend containers don't port-conflict with each other. To switch between configurations, set ENV_FILE before running make commands, (or docker-compose command): ENV_FILE=.env.off-pro make up # starts the OFF Producer's Platform containers. ENV_FILE=.env.obf make up # starts the OBF containers. ENV_FILE=.env.opff make up # starts the OPFF containers. or export it to keep it for a while: export ENV_FILE=.env.off # going to work on OFF for a while make up make restart make down make log A good strategy is to have multiple terminals open, one for each deployment: off [Terminal 1]: export ENV_FILE=.env.off make up off-pro [Terminal 2]: export ENV_FILE=.env.off-pro make up obf [Terminal 3]: export ENV_FILE=.env.obf make up opff [Terminal 3]: export ENV_FILE=.env.opff make up Note: the above case of 4 deployments is a bit ambitious , since ProductOpener's backend container takes about ~6GB of RAM to run, meaning that the above 4 deployments would require a total of 24GB of RAM available.","title":"different .env file"},{"location":"how-to-guides/ecoscore/","text":"Ecoscore and Agribalyse # Open Food Facts calculates the Ecoscore of a product from the Categories taxonomy where this has been linked to an AGRIBALYSE food code or proxy. New versions of the AGRIBALYSE database are released from time to time and this document explains how to apply updates. The high-level steps are as follows: Obtain and Convert the AGRIBALYSE Spreadsheet # Download the AGRIBALYSE food spreadsheet from the AGRIBALYSE web site (use the French site rather than English as updates on the English site may be delayed), and save it as AGRIBALYSE_vf.xlsm\" in the ecoscore/agribalyse folder. In a backend shell run the ssconvert.sh script. This will re-generate the CSV files, including the AGRIBALYSE_version and AGRIBALYSE_summary files. The AGRIBALYSE_summary file is sorted to make for easier comparison with the previous version. The Ecoscore calculation just uses the data from the \"Detail etape\" tab, which is converted to AGRIBALYSE_vf.csv.2 by ssconvert. The Ecoscore.pm module skips the first three lines of this file to ignore headers. This should be checked for each update as the number of header lines has previously changed. Also check that none of the column headings have changed. Review and fix any changed Categories # Review the changes to AGRIBALYSE_summary to determine if any codes have been removed or significantly edited and update the Categories taxonomy accordingly. Once the Categories have been updated you will need to build the taxonomies. You can then update unit test results with the update_tests_results.sh script to see if any have been affected. It is also worth checking the impact the update has had on the main product database. This can be downloaded locally and the differences determined by running the update_all_produycts script. The previous values of the Ecoscore are stored in the previous_data section under ecoscore_data. Before applying an update you will need to delete this section with the following MongoDB script: db . products . update ({}, { $unset : { \"ecoscore_data.previous_data\" : 0 }}); You can then use the following script from a backend bash shell to update products: ./update_all_products.pl --fields categories --compute-ecoscore The process will set the en:ecoscore_grade_changed and en:ecoscore_changed misc_tags, which can be queried to analyse the results. For example, the following script generates a CSV file that summaries all the categories where the grade has changed: var results = db . products . aggregate ([ { $match : { misc_tags : \"en:ecoscore-grade-changed\" } }, { $group : { _id : { en : \"$ecoscore_data.agribalyse.name_en\" , fr : \"$ecoscore_data.agribalyse.name_fr\" , code_before : \"$ecoscore_data.previous_data.agribalyse.code\" , code_after : \"$ecoscore_data.agribalyse.code\" , before : \"$ecoscore_data.previous_data.grade\" , after : \"$ecoscore_data.grade\" }, count : { $sum : 1 } } } ]). toArray (); print ( 'en.Name,fr.Name,Code Before,Code After,Grade Before,Grade After,Count' ); results . forEach (( result ) => { // eslint-disable-next-line no-underscore-dangle var id = result . _id ; print ( '\"' + ( id . en || '' ). replace ( /\"/g , '\"\"' ) + '\",\"' + ( id . fr || '' ). replace ( /\"/g , '\"\"' ) + '\",' + id . code_before + ',' + id . code_after + ',' + id . before + ',' + id . after + ',' + result . count ); }); The following script fetches the specific products that have changed: var products = db . products . find ( { misc_tags : \"en:ecoscore-grade-changed\" }, { _id : 1 , \"ecoscore_data.agribalyse.name_en\" : 1 , \"ecoscore_data.agribalyse.name_fr\" : 1 , \"ecoscore_data_main.agribalyse.code\" : 1 , \"ecoscore_data.previous_data.agribalyse.code\" : 1 , \"ecoscore_data.agribalyse.code\" : 1 , \"ecoscore_data_main.grade\" : 1 , \"ecoscore_data.previous_data.grade\" : 1 , \"ecoscore_data.grade\" : 1 , \"ecoscore_data_main.score\" : 1 , \"ecoscore_data.previous_data.score\" : 1 , \"ecoscore_data.score\" : 1 , \"ecoscore_data_main.agribalyse.ef_total\" : 1 , \"ecoscore_data.previous_data.agribalyse.ef_total\" : 1 , \"ecoscore_data.agribalyse.ef_total\" : 1 , \"categories_tags\" : 1 }). toArray (); print ( '_id,en.Name,fr.Name,Code Before Main,Code Before Change,Code After,Grade Before Main,Grade Before Change,Grade After,Score Before Main,Score Before Change,Score After,ef_total Before Main,ef_total Before Change,ef_total After,Categories Tags' ); products . forEach (( result ) => { var ecoscore_data_main = result . ecoscore_data_main || {}; var ecoscore_data_main_agribalyse = ecoscore_data_main . agribalyse || {}; // eslint-disable-next-line no-underscore-dangle print ( result . _id + ',\"' + ( result . ecoscore_data . agribalyse . name_en || '' ). replace ( /\"/g , '\"\"' ) + '\",\"' + ( result . ecoscore_data . agribalyse . name_fr || '' ). replace ( /\"/g , '\"\"' ) + '\",' + ecoscore_data_main_agribalyse . code + ',' + result . ecoscore_data . previous_data . agribalyse . code + ',' + result . ecoscore_data . agribalyse . code + ',' + ecoscore_data_main . grade + ',' + result . ecoscore_data . previous_data . grade + ',' + result . ecoscore_data . grade + ',' + ecoscore_data_main . score + ',' + result . ecoscore_data . previous_data . score + ',' + result . ecoscore_data . score + ',' + ecoscore_data_main_agribalyse . ef_total + ',' + result . ecoscore_data . previous_data . agribalyse . ef_total + ',' + result . ecoscore_data . agribalyse . ef_total + ',\"' + result . categories_tags . join ( \" \" ) + '\"' ); }); Link existing Categories to new AGRIBALYSE codes # If a new AGRIBALYSE category matches and existing OFF Category then the two can be linked by adding an agribalyse_food_code:en tag. If there is not a precise match then add an agribalyse_proxy_food_code:en tag along with the agribalyse_proxy_food_name:en and agribalyse_proxy_food_name:fr tags. Re-run the update_all_products script after doing this to assess how many products now have an Ecoscore when they did not previously. Use the above scripts to analyse the MongoDB, the new categories will have previous values of undefined . Add new Categories for new AGRIBALYSE codes # For any new categories, review the AGRIBALYSE category descriptions to ensure they are concise and unambiguous sucgh that an OFF user is most likely to get a match on a type-ahead search. Give notice of the change on the taxonomies channel in Slack so that additional translations can be added for the new categories. It is not necessary to add a category for every single AGRIBALYSE entry. For example, AGRIBALYSE has over 80 codes for different mineral waters but these all have almost exactly the same environmental impact. In cases like this it is acceptable to pick a single representative AGRIBALYSE code as a proxy for the Category in general. It may be worth doing a final check to see how many categories cominations still do not have a match to AGRIBALYSE: var missing = db . products . aggregate ([ { $match : { \"ecoscore_data.grade\" : null } }, { $group : { _id : \"$categories_tags\" , count : { $sum : 1 } } } ]). toArray (); print ( 'Category,Count' ); missing . forEach (( result ) => { // eslint-disable-next-line no-underscore-dangle var id = result . _id ; print ( '\"' + ( id . join ( ',' ) || '' ). replace ( /\"/g , '\"\"' ) + '\",' + result . count ); });","title":"Ecoscore and Agribalyse"},{"location":"how-to-guides/ecoscore/#ecoscore-and-agribalyse","text":"Open Food Facts calculates the Ecoscore of a product from the Categories taxonomy where this has been linked to an AGRIBALYSE food code or proxy. New versions of the AGRIBALYSE database are released from time to time and this document explains how to apply updates. The high-level steps are as follows:","title":"Ecoscore and Agribalyse"},{"location":"how-to-guides/ecoscore/#obtain-and-convert-the-agribalyse-spreadsheet","text":"Download the AGRIBALYSE food spreadsheet from the AGRIBALYSE web site (use the French site rather than English as updates on the English site may be delayed), and save it as AGRIBALYSE_vf.xlsm\" in the ecoscore/agribalyse folder. In a backend shell run the ssconvert.sh script. This will re-generate the CSV files, including the AGRIBALYSE_version and AGRIBALYSE_summary files. The AGRIBALYSE_summary file is sorted to make for easier comparison with the previous version. The Ecoscore calculation just uses the data from the \"Detail etape\" tab, which is converted to AGRIBALYSE_vf.csv.2 by ssconvert. The Ecoscore.pm module skips the first three lines of this file to ignore headers. This should be checked for each update as the number of header lines has previously changed. Also check that none of the column headings have changed.","title":"Obtain and Convert the AGRIBALYSE Spreadsheet"},{"location":"how-to-guides/ecoscore/#review-and-fix-any-changed-categories","text":"Review the changes to AGRIBALYSE_summary to determine if any codes have been removed or significantly edited and update the Categories taxonomy accordingly. Once the Categories have been updated you will need to build the taxonomies. You can then update unit test results with the update_tests_results.sh script to see if any have been affected. It is also worth checking the impact the update has had on the main product database. This can be downloaded locally and the differences determined by running the update_all_produycts script. The previous values of the Ecoscore are stored in the previous_data section under ecoscore_data. Before applying an update you will need to delete this section with the following MongoDB script: db . products . update ({}, { $unset : { \"ecoscore_data.previous_data\" : 0 }}); You can then use the following script from a backend bash shell to update products: ./update_all_products.pl --fields categories --compute-ecoscore The process will set the en:ecoscore_grade_changed and en:ecoscore_changed misc_tags, which can be queried to analyse the results. For example, the following script generates a CSV file that summaries all the categories where the grade has changed: var results = db . products . aggregate ([ { $match : { misc_tags : \"en:ecoscore-grade-changed\" } }, { $group : { _id : { en : \"$ecoscore_data.agribalyse.name_en\" , fr : \"$ecoscore_data.agribalyse.name_fr\" , code_before : \"$ecoscore_data.previous_data.agribalyse.code\" , code_after : \"$ecoscore_data.agribalyse.code\" , before : \"$ecoscore_data.previous_data.grade\" , after : \"$ecoscore_data.grade\" }, count : { $sum : 1 } } } ]). toArray (); print ( 'en.Name,fr.Name,Code Before,Code After,Grade Before,Grade After,Count' ); results . forEach (( result ) => { // eslint-disable-next-line no-underscore-dangle var id = result . _id ; print ( '\"' + ( id . en || '' ). replace ( /\"/g , '\"\"' ) + '\",\"' + ( id . fr || '' ). replace ( /\"/g , '\"\"' ) + '\",' + id . code_before + ',' + id . code_after + ',' + id . before + ',' + id . after + ',' + result . count ); }); The following script fetches the specific products that have changed: var products = db . products . find ( { misc_tags : \"en:ecoscore-grade-changed\" }, { _id : 1 , \"ecoscore_data.agribalyse.name_en\" : 1 , \"ecoscore_data.agribalyse.name_fr\" : 1 , \"ecoscore_data_main.agribalyse.code\" : 1 , \"ecoscore_data.previous_data.agribalyse.code\" : 1 , \"ecoscore_data.agribalyse.code\" : 1 , \"ecoscore_data_main.grade\" : 1 , \"ecoscore_data.previous_data.grade\" : 1 , \"ecoscore_data.grade\" : 1 , \"ecoscore_data_main.score\" : 1 , \"ecoscore_data.previous_data.score\" : 1 , \"ecoscore_data.score\" : 1 , \"ecoscore_data_main.agribalyse.ef_total\" : 1 , \"ecoscore_data.previous_data.agribalyse.ef_total\" : 1 , \"ecoscore_data.agribalyse.ef_total\" : 1 , \"categories_tags\" : 1 }). toArray (); print ( '_id,en.Name,fr.Name,Code Before Main,Code Before Change,Code After,Grade Before Main,Grade Before Change,Grade After,Score Before Main,Score Before Change,Score After,ef_total Before Main,ef_total Before Change,ef_total After,Categories Tags' ); products . forEach (( result ) => { var ecoscore_data_main = result . ecoscore_data_main || {}; var ecoscore_data_main_agribalyse = ecoscore_data_main . agribalyse || {}; // eslint-disable-next-line no-underscore-dangle print ( result . _id + ',\"' + ( result . ecoscore_data . agribalyse . name_en || '' ). replace ( /\"/g , '\"\"' ) + '\",\"' + ( result . ecoscore_data . agribalyse . name_fr || '' ). replace ( /\"/g , '\"\"' ) + '\",' + ecoscore_data_main_agribalyse . code + ',' + result . ecoscore_data . previous_data . agribalyse . code + ',' + result . ecoscore_data . agribalyse . code + ',' + ecoscore_data_main . grade + ',' + result . ecoscore_data . previous_data . grade + ',' + result . ecoscore_data . grade + ',' + ecoscore_data_main . score + ',' + result . ecoscore_data . previous_data . score + ',' + result . ecoscore_data . score + ',' + ecoscore_data_main_agribalyse . ef_total + ',' + result . ecoscore_data . previous_data . agribalyse . ef_total + ',' + result . ecoscore_data . agribalyse . ef_total + ',\"' + result . categories_tags . join ( \" \" ) + '\"' ); });","title":"Review and fix any changed Categories"},{"location":"how-to-guides/ecoscore/#link-existing-categories-to-new-agribalyse-codes","text":"If a new AGRIBALYSE category matches and existing OFF Category then the two can be linked by adding an agribalyse_food_code:en tag. If there is not a precise match then add an agribalyse_proxy_food_code:en tag along with the agribalyse_proxy_food_name:en and agribalyse_proxy_food_name:fr tags. Re-run the update_all_products script after doing this to assess how many products now have an Ecoscore when they did not previously. Use the above scripts to analyse the MongoDB, the new categories will have previous values of undefined .","title":"Link existing Categories to new AGRIBALYSE codes"},{"location":"how-to-guides/ecoscore/#add-new-categories-for-new-agribalyse-codes","text":"For any new categories, review the AGRIBALYSE category descriptions to ensure they are concise and unambiguous sucgh that an OFF user is most likely to get a match on a type-ahead search. Give notice of the change on the taxonomies channel in Slack so that additional translations can be added for the new categories. It is not necessary to add a category for every single AGRIBALYSE entry. For example, AGRIBALYSE has over 80 codes for different mineral waters but these all have almost exactly the same environmental impact. In cases like this it is acceptable to pick a single representative AGRIBALYSE code as a proxy for the Category in general. It may be worth doing a final check to see how many categories cominations still do not have a match to AGRIBALYSE: var missing = db . products . aggregate ([ { $match : { \"ecoscore_data.grade\" : null } }, { $group : { _id : \"$categories_tags\" , count : { $sum : 1 } } } ]). toArray (); print ( 'Category,Count' ); missing . forEach (( result ) => { // eslint-disable-next-line no-underscore-dangle var id = result . _id ; print ( '\"' + ( id . join ( ',' ) || '' ). replace ( /\"/g , '\"\"' ) + '\",' + result . count ); });","title":"Add new Categories for new AGRIBALYSE codes"},{"location":"how-to-guides/pro-development/","text":"Developing on the producers platform # Here is how to develop for the producers platform using docker. It suppose you already have setup docker for dev . You should have two kind of shell: - the shell for openfoodfacts - the shell for openfoodfacts-pro, this is a shell where you have source the setenv-pro.sh , that is you run . setenv-pro.sh . Your prompt, should now contains a (pro) to recall you you are in producers environment. (this simply sets some environment variables that will overides the one in .env) To develop, on producers plateform, you can then us a shell for openfoodfacts-pro and simply do a make dev and everything as usual. If you need to work on product import/export, or interacting with public platform, you have to start postgres and the minion on off side. That is, in a non pro shell, run docker-compose up postgres minion mongodb . Note that the setup does not currently support running the http server for both public and pro platform at the same time. So as you need the public platform: - in your pro shell , run a docker-compose stop backend - in your non pro shell , run a docker-compose up backend Now openfoodfacts.localhost is the public database. Of course, do this inside-out to access the pro http server. Note that if you use direnv , it should be fine, if you did not redefine variables set by setenv-pro.sh . An explanation of the setup can be found at pro-dev-setup.md If you want to see state of tasks, you can run: docker-compose exec minion /opt/product-opener/scripts/minion.pl minion job (add --help to see all options), or refer to https://docs.mojolicious.org/Minion/Command/minion/job You may also inspect database by running: docker-compose exec postgres psql -U productopener -W minion (password is given by POSTGRES_PASSWORD in .env and defaults to productopener ) Inspecting table minion, should help.","title":"Developing on the producers platform"},{"location":"how-to-guides/pro-development/#developing-on-the-producers-platform","text":"Here is how to develop for the producers platform using docker. It suppose you already have setup docker for dev . You should have two kind of shell: - the shell for openfoodfacts - the shell for openfoodfacts-pro, this is a shell where you have source the setenv-pro.sh , that is you run . setenv-pro.sh . Your prompt, should now contains a (pro) to recall you you are in producers environment. (this simply sets some environment variables that will overides the one in .env) To develop, on producers plateform, you can then us a shell for openfoodfacts-pro and simply do a make dev and everything as usual. If you need to work on product import/export, or interacting with public platform, you have to start postgres and the minion on off side. That is, in a non pro shell, run docker-compose up postgres minion mongodb . Note that the setup does not currently support running the http server for both public and pro platform at the same time. So as you need the public platform: - in your pro shell , run a docker-compose stop backend - in your non pro shell , run a docker-compose up backend Now openfoodfacts.localhost is the public database. Of course, do this inside-out to access the pro http server. Note that if you use direnv , it should be fine, if you did not redefine variables set by setenv-pro.sh . An explanation of the setup can be found at pro-dev-setup.md If you want to see state of tasks, you can run: docker-compose exec minion /opt/product-opener/scripts/minion.pl minion job (add --help to see all options), or refer to https://docs.mojolicious.org/Minion/Command/minion/job You may also inspect database by running: docker-compose exec postgres psql -U productopener -W minion (password is given by POSTGRES_PASSWORD in .env and defaults to productopener ) Inspecting table minion, should help.","title":"Developing on the producers platform"},{"location":"how-to-guides/use-direnv/","text":"Use direnv # As a developer, it can be better not to think too much about setting right env variables as you enter a project. direnv aims at providing a solution. As a quick guide as an openfoodfacts developer: install direnv on your system using usual package manager in your .bashrc add: # direnv eval \" $( direnv hook bash ) \" you have adapt the direnv line according to what you use, see direnv doc In your project directory add a file, where you superseed variables from .env that you wan't to echo \"setting up docker-compose env\" export DOCKER_BUILDKIT=1 export USER_UID=${UID} export USER_UID=$(id -g) in project directory, run direnv allow . in a new shell: go in project directory you should have direnv trigger and load your variables","title":"Use direnv"},{"location":"how-to-guides/use-direnv/#use-direnv","text":"As a developer, it can be better not to think too much about setting right env variables as you enter a project. direnv aims at providing a solution. As a quick guide as an openfoodfacts developer: install direnv on your system using usual package manager in your .bashrc add: # direnv eval \" $( direnv hook bash ) \" you have adapt the direnv line according to what you use, see direnv doc In your project directory add a file, where you superseed variables from .env that you wan't to echo \"setting up docker-compose env\" export DOCKER_BUILDKIT=1 export USER_UID=${UID} export USER_UID=$(id -g) in project directory, run direnv allow . in a new shell: go in project directory you should have direnv trigger and load your variables","title":"Use direnv"},{"location":"how-to-guides/use-gitpod/","text":"Using Gitpod for Remote Development # Gitpod provides powerful ready-to-code developer environments in the cloud eliminating the friction of setting up local environments and IDEs with Perl, Docker and plugins, making it possible for even new contributors to OpenFoodFacts Server to get started in minutes instead of hours! Note that while this how-to is tailored for Gitpod, using alternatives like GitHub Codespaces should be similar. For the most part, development on Gitpod is similar to developing locally as documented in the quickstart guide and docker-developer-guide , however accessing your dev-deployment of openfoodfacts-server requires an extra step. Get Started # Gitpod will automatically clone and open the repository for you in VSCode by default. It will also automatically build the project for you on opening and comes with Docker and other tools pre-installed making it one of the fastest ways to spin up an environment for openfoodfacts-server . Once the repository is open in Gitpod, other instructions in the quick-start guide can be generally followed. Accessing your development instance of OpenFoodFacts Web # Since Gitpod runs your code in a remote machine, your dev-deployment spun up with make dev or make up will not accessible when you open the default http://openfoodfacts.localhost in your browser. This occurs because the server running on the remote machine is not accessible on your local network interface. To overcome this, we can make use of SSH tunnel that listens to your local port 80 and forwards traffic to the port 80 of the remote machine. Gitpod makes it really simple to SSH into your dev environment by letting you copy the ssh command required to reach your remote environment. To start, follow the ssh instructions on Gitpod's official guide: SSH for workspaces as easy as copy/paste . Once you have copied the ssh command and ensure it works as-is, add a -L 80:localhost:80 to the command to make it look like: ssh -L 80:localhost:80 'openfoodfac-openfoodfac-tok-openfoodfac-r9f61214h9vt.ssh.ws-c.gitpod.io' . Once you execute the altered command in your terminal, you should be able to access OpenFoodFacts on http://openfoodfacts.localhost just as documented in the quickstart guide!","title":"Using Gitpod for Remote Development"},{"location":"how-to-guides/use-gitpod/#using-gitpod-for-remote-development","text":"Gitpod provides powerful ready-to-code developer environments in the cloud eliminating the friction of setting up local environments and IDEs with Perl, Docker and plugins, making it possible for even new contributors to OpenFoodFacts Server to get started in minutes instead of hours! Note that while this how-to is tailored for Gitpod, using alternatives like GitHub Codespaces should be similar. For the most part, development on Gitpod is similar to developing locally as documented in the quickstart guide and docker-developer-guide , however accessing your dev-deployment of openfoodfacts-server requires an extra step.","title":"Using Gitpod for Remote Development"},{"location":"how-to-guides/use-gitpod/#get-started","text":"Gitpod will automatically clone and open the repository for you in VSCode by default. It will also automatically build the project for you on opening and comes with Docker and other tools pre-installed making it one of the fastest ways to spin up an environment for openfoodfacts-server . Once the repository is open in Gitpod, other instructions in the quick-start guide can be generally followed.","title":"Get Started"},{"location":"how-to-guides/use-gitpod/#accessing-your-development-instance-of-openfoodfacts-web","text":"Since Gitpod runs your code in a remote machine, your dev-deployment spun up with make dev or make up will not accessible when you open the default http://openfoodfacts.localhost in your browser. This occurs because the server running on the remote machine is not accessible on your local network interface. To overcome this, we can make use of SSH tunnel that listens to your local port 80 and forwards traffic to the port 80 of the remote machine. Gitpod makes it really simple to SSH into your dev environment by letting you copy the ssh command required to reach your remote environment. To start, follow the ssh instructions on Gitpod's official guide: SSH for workspaces as easy as copy/paste . Once you have copied the ssh command and ensure it works as-is, add a -L 80:localhost:80 to the command to make it look like: ssh -L 80:localhost:80 'openfoodfac-openfoodfac-tok-openfoodfac-r9f61214h9vt.ssh.ws-c.gitpod.io' . Once you execute the altered command in your terminal, you should be able to access OpenFoodFacts on http://openfoodfacts.localhost just as documented in the quickstart guide!","title":"Accessing your development instance of OpenFoodFacts Web"},{"location":"how-to-guides/use-repl/","text":"Using Repl # On your local dev instance, the \"backend\" container comes with Devel::REPL installed. Thanks to PERL5LIB variable which is already configured, you can load any module of ProductOpener from within it. Also it as the right Launch Repl # Just run docker-compose run --rm docker-compose re.pl If you want to access external services (like mongodb), do not forget to start them. Testing perl code # It can be a handy way to get your hand into perl by testing some code patterns, or seeing how they react. For example one can test a regular expression: $ my $text = \"Hello World\" ; Hello World $ $ text =~ /Hello (\\w+)/i World Reading a sto # Another use case is reading a sto file to see what it contains. Eg. for a user: $ use ProductOpener:: Store qw/:all/ ; $ my $user_id = \"xxxx\" ; $ my $user_ref = retrieve ( \"/mnt/podata/users/$user_id.sto\" );","title":"Using Repl"},{"location":"how-to-guides/use-repl/#using-repl","text":"On your local dev instance, the \"backend\" container comes with Devel::REPL installed. Thanks to PERL5LIB variable which is already configured, you can load any module of ProductOpener from within it. Also it as the right","title":"Using Repl"},{"location":"how-to-guides/use-repl/#launch-repl","text":"Just run docker-compose run --rm docker-compose re.pl If you want to access external services (like mongodb), do not forget to start them.","title":"Launch Repl"},{"location":"how-to-guides/use-repl/#testing-perl-code","text":"It can be a handy way to get your hand into perl by testing some code patterns, or seeing how they react. For example one can test a regular expression: $ my $text = \"Hello World\" ; Hello World $ $ text =~ /Hello (\\w+)/i World","title":"Testing perl code"},{"location":"how-to-guides/use-repl/#reading-a-sto","text":"Another use case is reading a sto file to see what it contains. Eg. for a user: $ use ProductOpener:: Store qw/:all/ ; $ my $user_id = \"xxxx\" ; $ my $user_ref = retrieve ( \"/mnt/podata/users/$user_id.sto\" );","title":"Reading a sto"},{"location":"how-to-guides/use-vscode/","text":"Using VSCode # VSCode (or better the open source version VSCodium ) may be used to edit files. Here are some useful tricks. Perlcritic # One way to have perlcritic work is the following: install the perlcritic extension add a perlcritic.sh at the root of your project with following content: #!/usr/bin/env bash . .envrc >/dev/null 2 > & 1 docker-compose run --rm --no-deps backend perlcritic \" $@ \" 2 >/dev/null the second line is useful only if you use direnv chmod +x perlcritic.sh patch perlcritic by editing its files, following sfodje/perlcritic issue #26 the edit perlcritic configuration in workspace to set those values: Executable: /home/alex/docker/off-server/perlcritic.sh Perl Language Server # The extension Language Server and Debugger is less easy to work with ! Note: This setup does not work yet, but might not be so far. It is probably due to https://github.com/richterger/Perl-LanguageServer/issues/131 install the extension add a script shell-into-appserver.sh in the project: #!/usr/bin/env bash declare -x PATH = $PATH :/usr/local/bin/ source .envrc COMMAND = $( echo \" $@ \" | sed 's/^.*perl /perl /' ) > & 2 echo \"launching $COMMAND \" docker-compose run --rm --no-deps -T -p 127 .0.0. 1 :13603:13603 backend $COMMAND Note: the second line is useful only if you use direnv chmod +x shell-into-appserver.sh Edit workspace settings to have those settings: \"perl\" : { \"enable\" : true , \"perlInc\" : [ \"/opt/product-opener/lib\" , \"/opt/perl/local/lib/perl5\" ], \"ignoreDirs\" : [ \"/opt/perl/local/lib/perl5\" , \". vscode\" ], \"fileFilter\" : [ \".pm\" , \".pl\" , \".t\" ], \"sshAddr\" : \"dummy\" , \"sshUser\" : \"dummy\" , \"sshCmd\" : \"./shell-into-appserver.sh\" , \"sshWorkspaceRoot\" : \"/opt/product-opener\" , \"logLevel\" : 2 }, Remote container ? # Note: at the moment we do not support the Remote Container extension. While we can consider using it, it has some drawback because not all the project is contained within the \"backend\" container. For example all that concern nodejs is in the \"frontend\" container. So it means making a quite complete Docker image on its own with all the tooling necessary.","title":"Using VSCode"},{"location":"how-to-guides/use-vscode/#using-vscode","text":"VSCode (or better the open source version VSCodium ) may be used to edit files. Here are some useful tricks.","title":"Using VSCode"},{"location":"how-to-guides/use-vscode/#perlcritic","text":"One way to have perlcritic work is the following: install the perlcritic extension add a perlcritic.sh at the root of your project with following content: #!/usr/bin/env bash . .envrc >/dev/null 2 > & 1 docker-compose run --rm --no-deps backend perlcritic \" $@ \" 2 >/dev/null the second line is useful only if you use direnv chmod +x perlcritic.sh patch perlcritic by editing its files, following sfodje/perlcritic issue #26 the edit perlcritic configuration in workspace to set those values: Executable: /home/alex/docker/off-server/perlcritic.sh","title":"Perlcritic"},{"location":"how-to-guides/use-vscode/#perl-language-server","text":"The extension Language Server and Debugger is less easy to work with ! Note: This setup does not work yet, but might not be so far. It is probably due to https://github.com/richterger/Perl-LanguageServer/issues/131 install the extension add a script shell-into-appserver.sh in the project: #!/usr/bin/env bash declare -x PATH = $PATH :/usr/local/bin/ source .envrc COMMAND = $( echo \" $@ \" | sed 's/^.*perl /perl /' ) > & 2 echo \"launching $COMMAND \" docker-compose run --rm --no-deps -T -p 127 .0.0. 1 :13603:13603 backend $COMMAND Note: the second line is useful only if you use direnv chmod +x shell-into-appserver.sh Edit workspace settings to have those settings: \"perl\" : { \"enable\" : true , \"perlInc\" : [ \"/opt/product-opener/lib\" , \"/opt/perl/local/lib/perl5\" ], \"ignoreDirs\" : [ \"/opt/perl/local/lib/perl5\" , \". vscode\" ], \"fileFilter\" : [ \".pm\" , \".pl\" , \".t\" ], \"sshAddr\" : \"dummy\" , \"sshUser\" : \"dummy\" , \"sshCmd\" : \"./shell-into-appserver.sh\" , \"sshWorkspaceRoot\" : \"/opt/product-opener\" , \"logLevel\" : 2 },","title":"Perl Language Server"},{"location":"how-to-guides/use-vscode/#remote-container","text":"Note: at the moment we do not support the Remote Container extension. While we can consider using it, it has some drawback because not all the project is contained within the \"backend\" container. For example all that concern nodejs is in the \"frontend\" container. So it means making a quite complete Docker image on its own with all the tooling necessary.","title":"Remote container ?"},{"location":"how-to-guides/using-pages-from-openfoodfacts-web/","text":"Using pages from openfoodfacts-web # To avoid messing product-opener repository with translations of web-pages, we moved most pages in openfoodfacts-web repository specificly in the lang/ directory. This repo only has a really minimal lang directory named lang-default. If you want to have all contents locally, you should first clone openfoodfacts-web repo locally, and then: if you are using docker, you can set the WEB_LANG_PATH env variable to a relative or absolute path leading to openfoodfacts-web lang directory. else, make symlink lang point to openfoodfacts-web lang directory.","title":"Using pages from openfoodfacts-web"},{"location":"how-to-guides/using-pages-from-openfoodfacts-web/#using-pages-from-openfoodfacts-web","text":"To avoid messing product-opener repository with translations of web-pages, we moved most pages in openfoodfacts-web repository specificly in the lang/ directory. This repo only has a really minimal lang directory named lang-default. If you want to have all contents locally, you should first clone openfoodfacts-web repo locally, and then: if you are using docker, you can set the WEB_LANG_PATH env variable to a relative or absolute path leading to openfoodfacts-web lang directory. else, make symlink lang point to openfoodfacts-web lang directory.","title":"Using pages from openfoodfacts-web"},{"location":"introduction/api/","text":"Open Food Facts API Documentation # Everything you need to know about Open Food Facts API. Overview # Open Food Facts is a food products database made by everyone, for everyone, that can help you make better food choices. Seeing it is open data, anyone can reuse it for any purpose. For example, you are building a nutrition app. The Open Food Facts API enables developers to add to the products database and retrieve information about existing products. You may use the API to build applications allowing users to contribute to the database and make healthier food choices. The current version of the API is 2 . Data in the Open Food Facts database is provided voluntarily by users who want to support the program. As a result, there are no assurances that the data is accurate, complete, or reliable. The user assumes the entire risk of using the data. Before You Start # The Open Food Facts database is available under the Open Database License . The individual contents of the database are available under the Database Contents License . Product images are available under the Creative Commons Attribution ShareAlike license. They may contain graphical elements subject to copyright or other rights that may, in some cases, be reproduced (quotation rights or fair use). Please read the Terms and conditions of use and reuse before reusing the data. We are interested in learning what the Open Food Facts data is used for. It is not mandatory, but we would very much appreciate it if you tell us about your reuses so that we can share them with the Open Food Facts community. How to Best Use the API # General principles # You can search for product information, including many useful computed values. If you can't get the information on a specific product, you can get your user to send photos and data that will then be processed by Open Food Facts AI and contributors to get the computed result you want to show them. You can also implement the complete flow so that they immediately get the result with some effort on their side. If your users do not expect a result immediately (e.g., Inventory apps) # Submit photos (front/nutrition/ingredients): the most painless thing for your users The Open Food Facts AI Robotoff will generate some derived data from the photos. Over time, other apps and the Open Food Facts community will fill the data gaps. If your users expect a result immediately (e.g., Nutrition apps) # Submit nutrition facts + category > get Nutri-Score Submit ingredients > get the NOVA group (about food ultra-processing), additives, allergens, normalized ingredients, vegan, vegetarian\u2026 Submit category + labels > soon get the Eco-Score (about environmental impact) Environment # The OpenFoodFacts API has two environments. Production: https://world.openfoodfacts.org Staging: https://world.openfoodfacts.net Consider using the staging environment if you are not in a production scenario. While testing your applications, make all API requests to the staging environment. This way, we can ensure the product database is safe. Warning : The staging environment has an extra level of authentication (username: off, password: off). When making API requests to staging, you may use https://off:off@world.openfoodfacts.net/ as the base URL to include the authentication. Authentication # All requests do not require authentication except for WRITE operations (Editing an Existing Product, Uploading images\u2026). Create an account on the Open Food Facts app . You then have to alternative: The preferred one: use the login API to get a session cookie and use this cookie in your subsequent request to be authenticated. Note however that the session must always be used from the same IP address, and that you have a maximum of session per user. If session conditions are too restrictive for your use case, include your account credentials as parameters for authenticated requests where user_id is your username and password is your password (do this on POST / PUT / DELETE request, not on GET) To allow users of your app to contribute without registering individual accounts on the Open Food Facts website, you can create a global account. This way, we know that these contributions came from your application. The account you create in the production environment will only work for requests in production. You need to create an account in the staging environment if you want to make authenticated requests in staging. Reference Documentation (OpenAPI) # We are building a complete OpenAPI reference. See the OpenAPI documentation An older doc is also available Tutorials # See Using OFF API tutorial which is a good introduction on how to use the API. Help # Try the FAQ - to answer most of your questions. Didn't get a satisfactory answer? Contact the Team on the #api Slack Channel. Report Bugs on the Open Food Facts Database. Do you have an issue or feature request? You can submit it here on GitHub . Are you interested in contributing to this project? See our Contribution Guidelines . SDKS # SDKs are available for specific languages to facilitate the usage of the API. We probably have a wrapper for your favorite programming language. If we do, you can use it and improve it. If we don't, you can help create it. They will let you consume data and let your users contribute new data. Open-source contributors develop our SDKs, and more contributions are welcome to improve these SDKs. You can start by checking the existing issues in their respective repositories. Warning : Before exploring any SDK, endeavor to read the Before You Start section . Also remember, in case of problem, to check the API Reference Documentation first to verify if the problem is in SDK implementation or in the API itself. Cordova DART , Published on pub.dev Elixir Go NodeJS PHP Laravel Python , Published on pyipi React Native Ruby Java RUST R","title":"Open Food Facts API Documentation"},{"location":"introduction/api/#open-food-facts-api-documentation","text":"Everything you need to know about Open Food Facts API.","title":"Open Food Facts API Documentation"},{"location":"introduction/api/#overview","text":"Open Food Facts is a food products database made by everyone, for everyone, that can help you make better food choices. Seeing it is open data, anyone can reuse it for any purpose. For example, you are building a nutrition app. The Open Food Facts API enables developers to add to the products database and retrieve information about existing products. You may use the API to build applications allowing users to contribute to the database and make healthier food choices. The current version of the API is 2 . Data in the Open Food Facts database is provided voluntarily by users who want to support the program. As a result, there are no assurances that the data is accurate, complete, or reliable. The user assumes the entire risk of using the data.","title":"Overview"},{"location":"introduction/api/#before-you-start","text":"The Open Food Facts database is available under the Open Database License . The individual contents of the database are available under the Database Contents License . Product images are available under the Creative Commons Attribution ShareAlike license. They may contain graphical elements subject to copyright or other rights that may, in some cases, be reproduced (quotation rights or fair use). Please read the Terms and conditions of use and reuse before reusing the data. We are interested in learning what the Open Food Facts data is used for. It is not mandatory, but we would very much appreciate it if you tell us about your reuses so that we can share them with the Open Food Facts community.","title":"Before You Start"},{"location":"introduction/api/#how-to-best-use-the-api","text":"","title":"How to Best Use the API"},{"location":"introduction/api/#general-principles","text":"You can search for product information, including many useful computed values. If you can't get the information on a specific product, you can get your user to send photos and data that will then be processed by Open Food Facts AI and contributors to get the computed result you want to show them. You can also implement the complete flow so that they immediately get the result with some effort on their side.","title":"General principles"},{"location":"introduction/api/#if-your-users-do-not-expect-a-result-immediately-eg-inventory-apps","text":"Submit photos (front/nutrition/ingredients): the most painless thing for your users The Open Food Facts AI Robotoff will generate some derived data from the photos. Over time, other apps and the Open Food Facts community will fill the data gaps.","title":"If your users do not expect a result immediately (e.g., Inventory apps)"},{"location":"introduction/api/#if-your-users-expect-a-result-immediately-eg-nutrition-apps","text":"Submit nutrition facts + category > get Nutri-Score Submit ingredients > get the NOVA group (about food ultra-processing), additives, allergens, normalized ingredients, vegan, vegetarian\u2026 Submit category + labels > soon get the Eco-Score (about environmental impact)","title":"If your users expect a result immediately (e.g., Nutrition apps)"},{"location":"introduction/api/#environment","text":"The OpenFoodFacts API has two environments. Production: https://world.openfoodfacts.org Staging: https://world.openfoodfacts.net Consider using the staging environment if you are not in a production scenario. While testing your applications, make all API requests to the staging environment. This way, we can ensure the product database is safe. Warning : The staging environment has an extra level of authentication (username: off, password: off). When making API requests to staging, you may use https://off:off@world.openfoodfacts.net/ as the base URL to include the authentication.","title":"Environment"},{"location":"introduction/api/#authentication","text":"All requests do not require authentication except for WRITE operations (Editing an Existing Product, Uploading images\u2026). Create an account on the Open Food Facts app . You then have to alternative: The preferred one: use the login API to get a session cookie and use this cookie in your subsequent request to be authenticated. Note however that the session must always be used from the same IP address, and that you have a maximum of session per user. If session conditions are too restrictive for your use case, include your account credentials as parameters for authenticated requests where user_id is your username and password is your password (do this on POST / PUT / DELETE request, not on GET) To allow users of your app to contribute without registering individual accounts on the Open Food Facts website, you can create a global account. This way, we know that these contributions came from your application. The account you create in the production environment will only work for requests in production. You need to create an account in the staging environment if you want to make authenticated requests in staging.","title":"Authentication"},{"location":"introduction/api/#reference-documentation-openapi","text":"We are building a complete OpenAPI reference. See the OpenAPI documentation An older doc is also available","title":"Reference Documentation (OpenAPI)"},{"location":"introduction/api/#tutorials","text":"See Using OFF API tutorial which is a good introduction on how to use the API.","title":"Tutorials"},{"location":"introduction/api/#help","text":"Try the FAQ - to answer most of your questions. Didn't get a satisfactory answer? Contact the Team on the #api Slack Channel. Report Bugs on the Open Food Facts Database. Do you have an issue or feature request? You can submit it here on GitHub . Are you interested in contributing to this project? See our Contribution Guidelines .","title":"Help"},{"location":"introduction/api/#sdks","text":"SDKs are available for specific languages to facilitate the usage of the API. We probably have a wrapper for your favorite programming language. If we do, you can use it and improve it. If we don't, you can help create it. They will let you consume data and let your users contribute new data. Open-source contributors develop our SDKs, and more contributions are welcome to improve these SDKs. You can start by checking the existing issues in their respective repositories. Warning : Before exploring any SDK, endeavor to read the Before You Start section . Also remember, in case of problem, to check the API Reference Documentation first to verify if the problem is in SDK implementation or in the API itself. Cordova DART , Published on pub.dev Elixir Go NodeJS PHP Laravel Python , Published on pyipi React Native Ruby Java RUST R","title":"SDKS"},{"location":"introduction/dev-environment-quick-start-guide/","text":"Dev environment quick start guide # This guide will allow you to rapidly build a ready-to-use development environment for Product Opener running in Docker. As an alternative to setting up your environment locally, follow the Gitpod how-to guide to instantly provision a ready-to-code development environment in the cloud. First setup time estimate is ~10min with the following specs: * 8 GB of RAM dedicated to Docker client * 6 cores dedicated to Docker client * 12 MB/s internet speed 1. Prerequisites # Docker is the easiest way to install the Open Food Facts server, play with it, and even modify the code. Docker provides an isolated environment, very close to a Virtual Machine. This environment contains everything required to launch the Open Food Facts server. There is no need to install Perl, Perl modules, Nginx, nor Apache separately. Installation steps: - Install Docker CE If you run e.g. Debian, don't forget to add your user to the docker group! - Install Docker Compose - Enable command-line completion Windows Prerequisites # When running with Windows, install Docker Desktop which will cover all of the above. The Make tasks use a number of Linux commands, such as rm and nproc, so it is recommeded to run Make commands from the Git Bash shell. In addition, the following need to be installed and included in the PATH: Make for Windows wget for windows (In order to download the full product database). The process of cloning the repository will create a number of symbolic links which require specific permissions under Windows. In order to do this you can use any one of these alternatives: Use an Administrative command prompt for all Git commands Completely disable UAC Specifically grant the Create symbolic links permission to your user 2. Clone the repository from GitHub # You must have a GitHub account if you want to contribute to Open Food Facts development, but it\u2019s not required if you just want to see how it works. Be aware Open Food Facts server takes more than 1.3 GB (2019/11). Choose your prefered way to clone, either: On Windows: # If you are running Docker on Windows, please use the following git clone command: git clone -c core.symlinks=true https://github.com/openfoodfacts/openfoodfacts-server.git or git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git On other systems: # git clone git@github.com:openfoodfacts/openfoodfacts-server.git or git clone https://github.com/openfoodfacts/openfoodfacts-server.git Go to the cloned directory: cd openfoodfacts-server/ 3. [Optional] Review Product Opener's environment # Note: you can skip this step for the first setup since the default .env in the repo contains all the default values required to get started. Before running the docker-compose deployment, you can review and configure Product Opener's environment ( .env file). The .env file contains ProductOpener default settings: | Field | Description | | ----------------------------------------------------------------- | --- | | PRODUCT_OPENER_DOMAIN | Can be set to different values based on which OFF flavor is run.| | PRODUCT_OPENER_PORT | can be modified to run NGINX on a different port. Useful when running multiple OFF flavors on different ports on the same host. Default port: 80 .| | PRODUCT_OPENER_FLAVOR | Can be modified to run different flavors of OpenFoodFacts, amongst openfoodfacts (default), openbeautyfacts , openpetfoodfacts , openproductsfacts .| | PRODUCT_OPENER_FLAVOR_SHORT | can be modified to run different flavors of OpenFoodFacts, amongst off (default), obf , oppf , opf .| | PRODUCERS_PLATFORM | can be set to 1 to build / run the producer platform .| | ROBOTOFF_URL | can be set to connect with a Robotoff instance .| | REDIS_URL | can be set to connect with a Redis instance for populating the search index .| | GOOGLE_CLOUD_VISION_API_KEY | can be set to enable OCR using Google Cloud Vision .| | CROWDIN_PROJECT_IDENTIFIER and CROWDIN_PROJECT_KEY | can be set to run translations .| | GEOLITE2_PATH , GEOLITE2_ACCOUNT_ID and GEOLITE2_LICENSE_KEY | can be set to enable Geolite2 .| | TAG | Is set to latest by default, but you can specify any Docker Hub tag for the frontend / backend images. Note that this is useful only if you use pre-built images from the Docker Hub ( docker/prod.yml override); the default dev setup ( docker/dev.yml ) builds images locally| The .env file also contains some useful Docker Compose variables: * COMPOSE_PROJECT_NAME is the compose project name that sets the prefix to every container name . Do not update this unless you know what you're doing. * COMPOSE_FILE is the ; -separated list of Docker compose files that are included in the deployment: * For a development -like environment, set it to docker-compose.yml;docker/dev.yml (default) * For a production -like environment, set it to docker-compose.yml;docker/prod.yml;docker/mongodb.yml * For more features, you can add: * docker/admin-uis.yml : add the Admin UIS container * docker/geolite2.yml : add the Geolite2 container * docker/perldb.yml : add the Perl debugger container * COMPOSE_SEPARATOR is the separator used for COMPOSE_FILE . Note: Instead of modifying .env (with the risk commit it inadvertently), You can also set needed variables in your shell, they will override .env values. Consider creating a .envrc file that you source each time you need to work on the project. On linux and macOS, you can automatically do it if you use direnv . 4. Build your dev environment # From the repository root, run: make dev Note: If you are using Windows, you may encounter issues regarding this command. Take a look at the Troubleshooting section further in this tutorial. Note: If docker complains about ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network It can be solved by adding {\"base\": \"172.80.0.0/16\",\"size\": 24}, {\"base\": \"172.90.0.0/16\",\"size\": 24} to default-address-pools in /etc/docker/daemon.json and then restarting the docker daemon. Credits to https://theorangeone.net/posts/increase-docker-ip-space/ for this solution. The command will run 2 subcommands: * make up : Build and run containers from the local directory and bind local code files, so that you do not have to rebuild everytime. * make import_sample_data : Load sample data into mongodb container (~100 products). Notes: The first build can take between 10 and 30 minutes depending on your machine and internet connection (broadband connection heavily recommended, as this will download Docker base images, install Debian and Perl modules in preparation of the final container image). You might not immediately see the test products: create an account, login, and they should appear. For a full description of available make targets, see docker/README.md Hosts file: Since the default PRODUCT_OPENER_DOMAIN in the .env file is set to openfoodfacts.localhost , add the following to your hosts file (Windows: C:\\Windows\\System32\\drivers\\etc\\hosts ; Linux/MacOSX: /etc/hosts ): 127.0.0.1 world.openfoodfacts.localhost fr.openfoodfacts.localhost static.openfoodfacts.localhost ssl-api.openfoodfacts.localhost fr-en.openfoodfacts.localhost You're done ! Check http://openfoodfacts.localhost/ ! Going further # To learn more about developing with Docker, see the Docker developer's guide . To have all site page on your dev instance, see Using pages from openfoodfacts-web Using Repl offers you a way to play with perl. Specific notes are provide on appying AGRIBALYSE updates to support the Ecoscore calculation. Visual Studio Code # WARNING : for now this is deprecated, some work needs to be done. This repository comes with a configuration for Visual Studio Code (VS Code) development containers (devcontainer) . This enables some Perl support in VS Code without the need to install the correct Perl version and modules on your local machine. To use the devcontainer, install prerequisites , clone the repository from GitHub , and (optionally) review Product Opener's environment . Additionally, install Visual Studio Code . VS Code will automatically recommend some extensions, but if you don't want to install all of them, please do install Remote - Containers manually. You can then use the extension command Remote-Containers: Reopen Folder in Container , which will automatically build the container and start the services. No need to use make ! Troubleshooting # make dev error: make: command not found # When running make dev : bash: make: command not found Solution: Click the Windows button, then type \u201cenvironment properties\u201d into the search bar and hit Enter. Click Environment Variables, then under System variables choose Path and click Edit. Click New and insert C:\\Program Files (x86)\\GnuWin32\\bin, then save the changes. Open a new terminal and test that the command works. (see Make Windows for more) make dev error: [build_lang] Error 2 - Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto # When running make dev : <h1>Software error:</h1> <pre>Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. </p> [Tue Apr 5 19:36:40 2022] build_lang.pl: Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. [Tue Apr 5 19:36:40 2022] build_lang.pl: Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. [Tue Apr 5 19:36:40 2022] build_lang.pl: BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. make: *** [build_lang] Error 2 Solution: Project needs Symlinks to be enabled. traces.result.sto is a symlink to allergens.result.sto On Windows, You have to enable the 'Developer Mode' in order to use the symlinks. To enable Developer Mode: Go under Settings > Update & Security > 'For developers', and turn on the toggle for Developer Mode. On Windows systems, the git repository needs to be cloned with symlinks enabled. You need to remove current directory where you clone the project, and clone the project again, using right options: git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git 'rm' is not recognized as an internal or external command # When running make import_prod_data or some other commands. Solution: Use the Git Bash shell to run the make commands in windows so that programs like nproc and rm are found. System cannot find wget # When running make import_prod_data . process_begin: CreateProcess(NULL, wget --no-verbose https://static.openfoodfacts.org/data/openfoodfacts-mongodbdump.tar.gz, ...) failed. make (e=2): The system cannot find the file specified. You need to install wget for windows . The referenced version is able to use the Windows Certificate Store, whereas the standard gnuwin32 version will give errors about not being able to verify the server certificate. make: *** [Makefile:154: import_sample_data] Error 22 # When running make import_sample_data <hl>Software error:</h1> <pre>MongoDB: :SelectionError: No writable server available. MongoDB server status: Topology type: Single; Member status: mongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution ) </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. <p> [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: MongoDB::SelectionError: No writable server available. MongoDB server status: [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: Topology type: Single; Member status: [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: mongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution ) make: *** [Makefile:154: import_sample data] Error 22 Solution: The cause of this issue is that you already have the mongodb database server running on your local machine at port 27017. For linux users: First stop the MongoDB server from your OS sudo systemctl stop mongod Then check that mongod is stopped with: systemctl status mongod | grep Active Note: The output of this command should be: Active: inactive (dead) Then, executed this: docker-compose up Note: To know more about docker-compose commands do read this guide","title":"Dev environment quick start guide"},{"location":"introduction/dev-environment-quick-start-guide/#dev-environment-quick-start-guide","text":"This guide will allow you to rapidly build a ready-to-use development environment for Product Opener running in Docker. As an alternative to setting up your environment locally, follow the Gitpod how-to guide to instantly provision a ready-to-code development environment in the cloud. First setup time estimate is ~10min with the following specs: * 8 GB of RAM dedicated to Docker client * 6 cores dedicated to Docker client * 12 MB/s internet speed","title":"Dev environment quick start guide"},{"location":"introduction/dev-environment-quick-start-guide/#1-prerequisites","text":"Docker is the easiest way to install the Open Food Facts server, play with it, and even modify the code. Docker provides an isolated environment, very close to a Virtual Machine. This environment contains everything required to launch the Open Food Facts server. There is no need to install Perl, Perl modules, Nginx, nor Apache separately. Installation steps: - Install Docker CE If you run e.g. Debian, don't forget to add your user to the docker group! - Install Docker Compose - Enable command-line completion","title":"1. Prerequisites"},{"location":"introduction/dev-environment-quick-start-guide/#windows-prerequisites","text":"When running with Windows, install Docker Desktop which will cover all of the above. The Make tasks use a number of Linux commands, such as rm and nproc, so it is recommeded to run Make commands from the Git Bash shell. In addition, the following need to be installed and included in the PATH: Make for Windows wget for windows (In order to download the full product database). The process of cloning the repository will create a number of symbolic links which require specific permissions under Windows. In order to do this you can use any one of these alternatives: Use an Administrative command prompt for all Git commands Completely disable UAC Specifically grant the Create symbolic links permission to your user","title":"Windows Prerequisites"},{"location":"introduction/dev-environment-quick-start-guide/#2-clone-the-repository-from-github","text":"You must have a GitHub account if you want to contribute to Open Food Facts development, but it\u2019s not required if you just want to see how it works. Be aware Open Food Facts server takes more than 1.3 GB (2019/11). Choose your prefered way to clone, either:","title":"2. Clone the repository from GitHub"},{"location":"introduction/dev-environment-quick-start-guide/#on-windows","text":"If you are running Docker on Windows, please use the following git clone command: git clone -c core.symlinks=true https://github.com/openfoodfacts/openfoodfacts-server.git or git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git","title":"On Windows:"},{"location":"introduction/dev-environment-quick-start-guide/#on-other-systems","text":"git clone git@github.com:openfoodfacts/openfoodfacts-server.git or git clone https://github.com/openfoodfacts/openfoodfacts-server.git Go to the cloned directory: cd openfoodfacts-server/","title":"On other systems:"},{"location":"introduction/dev-environment-quick-start-guide/#3-optional-review-product-openers-environment","text":"Note: you can skip this step for the first setup since the default .env in the repo contains all the default values required to get started. Before running the docker-compose deployment, you can review and configure Product Opener's environment ( .env file). The .env file contains ProductOpener default settings: | Field | Description | | ----------------------------------------------------------------- | --- | | PRODUCT_OPENER_DOMAIN | Can be set to different values based on which OFF flavor is run.| | PRODUCT_OPENER_PORT | can be modified to run NGINX on a different port. Useful when running multiple OFF flavors on different ports on the same host. Default port: 80 .| | PRODUCT_OPENER_FLAVOR | Can be modified to run different flavors of OpenFoodFacts, amongst openfoodfacts (default), openbeautyfacts , openpetfoodfacts , openproductsfacts .| | PRODUCT_OPENER_FLAVOR_SHORT | can be modified to run different flavors of OpenFoodFacts, amongst off (default), obf , oppf , opf .| | PRODUCERS_PLATFORM | can be set to 1 to build / run the producer platform .| | ROBOTOFF_URL | can be set to connect with a Robotoff instance .| | REDIS_URL | can be set to connect with a Redis instance for populating the search index .| | GOOGLE_CLOUD_VISION_API_KEY | can be set to enable OCR using Google Cloud Vision .| | CROWDIN_PROJECT_IDENTIFIER and CROWDIN_PROJECT_KEY | can be set to run translations .| | GEOLITE2_PATH , GEOLITE2_ACCOUNT_ID and GEOLITE2_LICENSE_KEY | can be set to enable Geolite2 .| | TAG | Is set to latest by default, but you can specify any Docker Hub tag for the frontend / backend images. Note that this is useful only if you use pre-built images from the Docker Hub ( docker/prod.yml override); the default dev setup ( docker/dev.yml ) builds images locally| The .env file also contains some useful Docker Compose variables: * COMPOSE_PROJECT_NAME is the compose project name that sets the prefix to every container name . Do not update this unless you know what you're doing. * COMPOSE_FILE is the ; -separated list of Docker compose files that are included in the deployment: * For a development -like environment, set it to docker-compose.yml;docker/dev.yml (default) * For a production -like environment, set it to docker-compose.yml;docker/prod.yml;docker/mongodb.yml * For more features, you can add: * docker/admin-uis.yml : add the Admin UIS container * docker/geolite2.yml : add the Geolite2 container * docker/perldb.yml : add the Perl debugger container * COMPOSE_SEPARATOR is the separator used for COMPOSE_FILE . Note: Instead of modifying .env (with the risk commit it inadvertently), You can also set needed variables in your shell, they will override .env values. Consider creating a .envrc file that you source each time you need to work on the project. On linux and macOS, you can automatically do it if you use direnv .","title":"3. [Optional] Review Product Opener's environment"},{"location":"introduction/dev-environment-quick-start-guide/#4-build-your-dev-environment","text":"From the repository root, run: make dev Note: If you are using Windows, you may encounter issues regarding this command. Take a look at the Troubleshooting section further in this tutorial. Note: If docker complains about ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network It can be solved by adding {\"base\": \"172.80.0.0/16\",\"size\": 24}, {\"base\": \"172.90.0.0/16\",\"size\": 24} to default-address-pools in /etc/docker/daemon.json and then restarting the docker daemon. Credits to https://theorangeone.net/posts/increase-docker-ip-space/ for this solution. The command will run 2 subcommands: * make up : Build and run containers from the local directory and bind local code files, so that you do not have to rebuild everytime. * make import_sample_data : Load sample data into mongodb container (~100 products). Notes: The first build can take between 10 and 30 minutes depending on your machine and internet connection (broadband connection heavily recommended, as this will download Docker base images, install Debian and Perl modules in preparation of the final container image). You might not immediately see the test products: create an account, login, and they should appear. For a full description of available make targets, see docker/README.md Hosts file: Since the default PRODUCT_OPENER_DOMAIN in the .env file is set to openfoodfacts.localhost , add the following to your hosts file (Windows: C:\\Windows\\System32\\drivers\\etc\\hosts ; Linux/MacOSX: /etc/hosts ): 127.0.0.1 world.openfoodfacts.localhost fr.openfoodfacts.localhost static.openfoodfacts.localhost ssl-api.openfoodfacts.localhost fr-en.openfoodfacts.localhost You're done ! Check http://openfoodfacts.localhost/ !","title":"4. Build your dev environment"},{"location":"introduction/dev-environment-quick-start-guide/#going-further","text":"To learn more about developing with Docker, see the Docker developer's guide . To have all site page on your dev instance, see Using pages from openfoodfacts-web Using Repl offers you a way to play with perl. Specific notes are provide on appying AGRIBALYSE updates to support the Ecoscore calculation.","title":"Going further"},{"location":"introduction/dev-environment-quick-start-guide/#visual-studio-code","text":"WARNING : for now this is deprecated, some work needs to be done. This repository comes with a configuration for Visual Studio Code (VS Code) development containers (devcontainer) . This enables some Perl support in VS Code without the need to install the correct Perl version and modules on your local machine. To use the devcontainer, install prerequisites , clone the repository from GitHub , and (optionally) review Product Opener's environment . Additionally, install Visual Studio Code . VS Code will automatically recommend some extensions, but if you don't want to install all of them, please do install Remote - Containers manually. You can then use the extension command Remote-Containers: Reopen Folder in Container , which will automatically build the container and start the services. No need to use make !","title":"Visual Studio Code"},{"location":"introduction/dev-environment-quick-start-guide/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"introduction/dev-environment-quick-start-guide/#make-dev-error-make-command-not-found","text":"When running make dev : bash: make: command not found Solution: Click the Windows button, then type \u201cenvironment properties\u201d into the search bar and hit Enter. Click Environment Variables, then under System variables choose Path and click Edit. Click New and insert C:\\Program Files (x86)\\GnuWin32\\bin, then save the changes. Open a new terminal and test that the command works. (see Make Windows for more)","title":"make dev error: make: command not found"},{"location":"introduction/dev-environment-quick-start-guide/#make-dev-error-build_lang-error-2-could-not-load-taxonomy-mntpodatataxonomiestracesresultsto","text":"When running make dev : <h1>Software error:</h1> <pre>Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. </p> [Tue Apr 5 19:36:40 2022] build_lang.pl: Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. [Tue Apr 5 19:36:40 2022] build_lang.pl: Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. [Tue Apr 5 19:36:40 2022] build_lang.pl: BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. make: *** [build_lang] Error 2 Solution: Project needs Symlinks to be enabled. traces.result.sto is a symlink to allergens.result.sto On Windows, You have to enable the 'Developer Mode' in order to use the symlinks. To enable Developer Mode: Go under Settings > Update & Security > 'For developers', and turn on the toggle for Developer Mode. On Windows systems, the git repository needs to be cloned with symlinks enabled. You need to remove current directory where you clone the project, and clone the project again, using right options: git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git","title":"make dev error: [build_lang] Error 2 - Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto"},{"location":"introduction/dev-environment-quick-start-guide/#rm-is-not-recognized-as-an-internal-or-external-command","text":"When running make import_prod_data or some other commands. Solution: Use the Git Bash shell to run the make commands in windows so that programs like nproc and rm are found.","title":"'rm' is not recognized as an internal or external command"},{"location":"introduction/dev-environment-quick-start-guide/#system-cannot-find-wget","text":"When running make import_prod_data . process_begin: CreateProcess(NULL, wget --no-verbose https://static.openfoodfacts.org/data/openfoodfacts-mongodbdump.tar.gz, ...) failed. make (e=2): The system cannot find the file specified. You need to install wget for windows . The referenced version is able to use the Windows Certificate Store, whereas the standard gnuwin32 version will give errors about not being able to verify the server certificate.","title":"System cannot find wget"},{"location":"introduction/dev-environment-quick-start-guide/#make-makefile154-import_sample_data-error-22","text":"When running make import_sample_data <hl>Software error:</h1> <pre>MongoDB: :SelectionError: No writable server available. MongoDB server status: Topology type: Single; Member status: mongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution ) </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. <p> [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: MongoDB::SelectionError: No writable server available. MongoDB server status: [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: Topology type: Single; Member status: [Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: mongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution ) make: *** [Makefile:154: import_sample data] Error 22 Solution: The cause of this issue is that you already have the mongodb database server running on your local machine at port 27017. For linux users: First stop the MongoDB server from your OS sudo systemctl stop mongod Then check that mongod is stopped with: systemctl status mongod | grep Active Note: The output of this command should be: Active: inactive (dead) Then, executed this: docker-compose up Note: To know more about docker-compose commands do read this guide","title":"make: *** [Makefile:154: import_sample_data] Error 22"},{"location":"introduction/knowledge-panels/","text":"Knowledge panels # The Open Food Facts API allows clients (such as the Open Food Facts website and mobile app) to request ready to display bits of information about an object (such as a product or a facet like a category). Clients do not have to know in advance what kind of information is displayed (e.g. the ingredients of a product, nutrition data, the Nutri-Score or the Eco-Score), they only have to know how to display basic types of data such as texts, grades, images, and tables. The structure of the knowledge panels data returned by the API is described in the panels JSON schema. See the reference documentation: https://openfoodfacts.github.io/openfoodfacts-server/reference/api/#tag/Read-Requests/operation/get-product-by-barcode-knowledge-panels See also the source file, containing json schema","title":"Knowledge panels"},{"location":"introduction/knowledge-panels/#knowledge-panels","text":"The Open Food Facts API allows clients (such as the Open Food Facts website and mobile app) to request ready to display bits of information about an object (such as a product or a facet like a category). Clients do not have to know in advance what kind of information is displayed (e.g. the ingredients of a product, nutrition data, the Nutri-Score or the Eco-Score), they only have to know how to display basic types of data such as texts, grades, images, and tables. The structure of the knowledge panels data returned by the API is described in the panels JSON schema. See the reference documentation: https://openfoodfacts.github.io/openfoodfacts-server/reference/api/#tag/Read-Requests/operation/get-product-by-barcode-knowledge-panels See also the source file, containing json schema","title":"Knowledge panels"},{"location":"reference/api-cheatsheet/","text":"Open Food Facts API - CheatSheet # This reference cheatsheet gives you a quick reminder to send requests to the OFF API. If you are new to API usage you might look at the tutorial . Also, refer to the API reference documentation for complete information. Add/Edit an Existing Product # Indicate the absence of nutrition facts # no_nutrition_data=on (indicates if the nutrition facts are not indicated on the food label) Add nutrition facts values, units and base # nutrition_data_per=100g OR nutrition_data_per=serving serving_size=38g nutriment_energy=450 nutriment_energy_unit=kJ Adding values to a field that is already filled # You just have to prefix add_ before the name of the field add_categories add_labels add_brands","title":"Open Food Facts API - CheatSheet"},{"location":"reference/api-cheatsheet/#open-food-facts-api-cheatsheet","text":"This reference cheatsheet gives you a quick reminder to send requests to the OFF API. If you are new to API usage you might look at the tutorial . Also, refer to the API reference documentation for complete information.","title":"Open Food Facts API - CheatSheet"},{"location":"reference/api-cheatsheet/#addedit-an-existing-product","text":"","title":"Add/Edit an Existing Product"},{"location":"reference/api-cheatsheet/#indicate-the-absence-of-nutrition-facts","text":"no_nutrition_data=on (indicates if the nutrition facts are not indicated on the food label)","title":"Indicate the absence of nutrition facts"},{"location":"reference/api-cheatsheet/#add-nutrition-facts-values-units-and-base","text":"nutrition_data_per=100g OR nutrition_data_per=serving serving_size=38g nutriment_energy=450 nutriment_energy_unit=kJ","title":"Add nutrition facts values, units and base"},{"location":"reference/api-cheatsheet/#adding-values-to-a-field-that-is-already-filled","text":"You just have to prefix add_ before the name of the field add_categories add_labels add_brands","title":"Adding values to a field that is already filled"},{"location":"reference/api-v3/","text":"v3 OpenAPI documentation # See api-v3.yml for edition. Do not write anything here, it is meant to be overwritten by html generated from api-v3.yml","title":"v3 OpenAPI documentation"},{"location":"reference/api-v3/#v3-openapi-documentation","text":"See api-v3.yml for edition. Do not write anything here, it is meant to be overwritten by html generated from api-v3.yml","title":"v3 OpenAPI documentation"},{"location":"reference/api/","text":"OpenAPI documentation # See api.yml for edition. Do not write anything here, it is meant to be overwritten by html generated from api.yml","title":"OpenAPI documentation"},{"location":"reference/api/#openapi-documentation","text":"See api.yml for edition. Do not write anything here, it is meant to be overwritten by html generated from api.yml","title":"OpenAPI documentation"},{"location":"reference/perl/","text":"Perl reference documentation # The documentation in Plain Old Format (aka POD) for perl module is compiled from in file documentation. See the Perl reference documentation","title":"Perl reference documentation"},{"location":"reference/perl/#perl-reference-documentation","text":"The documentation in Plain Old Format (aka POD) for perl module is compiled from in file documentation. See the Perl reference documentation","title":"Perl reference documentation"},{"location":"tutorials/using-the-OFF-API-tutorial/","text":"Using the Open Food Facts API # Scan A Product To Get Nutri-score # This basic tutorial shows you can get the Nutri-score of a product, for instance, to display it in a mobile app after scanning the product barcode. Let's use Nutella Ferrero as the product example for this tutorial. To get a product nutriscore, you need to make a call to the Get A Product By Barcode Endpoint. Authentication # Usually, no authentication is required to query Get A Product Nutri-score. However, on the staging environment(which is used throughout this tutorial), there is a basic auth to avoid content indexation. For more details, visit the Open Food Facts API Environment . Describing the Get Request # Make a GET request to the Get A Product By Barcode endpoint. https://world.openfoodfacts.net/api/v2/product/{barcode} The {barcode} is the barcode number of the product you are trying to get. The barcode for Nutella Ferrero is 3017624010701 . Then the request path to get product data for Nutella Ferrero will look like this: https://world.openfoodfacts.net/api/v2/product/3017624010701 The response returns every data about Nutella Ferrero on the database. To get the nutriscore, we need to limit the response by specifying the nutriscore field, which is the nutrition_grades and product_name . Query Parameters # To limit the response of the Get A Product By Barcode response, use query parameters to specify the product fields to be returned. In this example, you need one query parameter called field with the value product_name,nutrition_grades . The request path will now look like this: https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data Nutri-Score Response # The response returned contains an object of the code , product , status_verbose , and status . The product object contains the fields specified in the query: the product_name and the nutrition_grades . The status also states if the product was found or not. { \"code\" : \"3017624010701\" , \"product\" : { \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } Nutri-Score Computation # If you would like to be able to show how the score is computed, add some extra fields like nutriscore_data and nutriments . The request path to get the Nutri-Score computation for Nutella-Ferroro will be : https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data,nutriments,nutrition_grades The product object in the response now contains the extra fields to show how the nutriscore was computed. { \"code\" : \"3017624010701\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 57.5 , \"carbohydrates_100g\" : 57.5 , \"carbohydrates_unit\" : \"g\" , \"carbohydrates_value\" : 57.5 , \"energy\" : 2255 , \"energy-kcal\" : 539 , \"energy-kcal_100g\" : 539 , \"energy-kcal_unit\" : \"kcal\" , ... , ... , \"sugars\" : 56.3 , \"sugars_100g\" : 56.3 , \"sugars_unit\" : \"g\" , \"sugars_value\" : 56.3 }, \"nutriscore_data\" : { \"energy\" : 2255 , \"energy_points\" : 6 , \"energy_value\" : 2255 , ... , ... , \"sugars_points\" : 10 , \"sugars_value\" : 56.3 }, \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Get A Product By Barcode Completing products to get the Nutri-Score # Products without a Nutri-Score # When these fields are missing in a nutriscore computation response, it means that the product does not have a Nutri-Score computation due to some missing nutrition data. Lets look at the 100% Real Orange Juice . If the product nutrition data is missing some fields, you can volunteer and contribute to it by getting the missing tags and writing to the OFF API to add them. To know the missing tags, you need the misc-tags field from the response. https://world.openfoodfacts.net/api/v2/product/0180411000803/100-real-orange-juice?fields=misc_tags The response shows the missing fields and category needed to compute the Nutri-Score. { \"code\" : \"0180411000803\" , \"product\" : { \"misc_tags\" : [ \"en:nutriscore-not-computed\" , \"en:nutriscore-missing-category\" , \"en:nutrition-not-enough-data-to-compute-nutrition-score\" , \"en:nutriscore-missing-nutrition-data\" , \"en:nutriscore-missing-nutrition-data-sodium\" , \"en:ecoscore-extended-data-not-computed\" , \"en:ecoscore-not-computed\" , \"en:main-countries-new-product\" ] }, \"status\" : 1 , \"status_verbose\" : \"product found\" } The sample response above for 100% Real Orange Juice misc_tags shows that the Nutri-Score is missing category ( en:nutriscore-missing-category ) and sodium(salt) ( en:nutriscore-missing-nutrition-data-sodium ). Now you can write to the OFF API to provide these nutriment data (if you have them) so that the Nutri-Score can be computed. Write data to make Nutri-Score computation possible # The WRITE operations in the OFF API require authentication, therefore you need a valid user_id and password to write the missing nutriment data to 100% Real Orange Juice. Sign up on the Open Food Facts App , to get your user_id and password if you dont have. To write data to a product, make a POST request to the Add or Edit A Product endpoint. https://world.openfoodfacts.net/cgi/product_jqm2.pl For authentication, add your valid user_id and password as body parameters to your request. The code (barcode of the product to be added/edited), user_id and password are required fields when adding or editing a product. Then, include other product data to be added in the request body. To write sodium and category to 100% Real Orange Juice so that the Nutri-Score can be computed, the request body should contain these fields : Key Value Description user_id *** A valid user_id password *** A valid password code 0180411000803 The barcode of the product to be added/edited nutriment_sodium 0.015 Amount of sodium nutriment_sodium_unit g Unit of sodium relative to the amount categories Orange Juice Category of the Product Using curl: curl -XPOST -u off:off -x POST https://world.openfoodfacts.net/cgi/product_jqm2.pl \\ -F user_id = your_user_id -F password = your_password \\ -F code = 0180411000803 -F nutriment_sodium = 0 .015 -F nutriment_sodium_unit = g -F categories = \"Orange Juice\" If the request is succesful, it returns a response that indicated that the fields have been saved. { \"status_verbose\" : \"fields saved\" , \"status\" : 1 } Read newly computed Nutri-Score # Now, let's check if the Nutri-Score for 100% Real Orange Juice has been computed now that we have provided the missing data. Make a GET request to https://world.openfoodfacts.net/api/v2/product/0180411000803?fields=product_name,nutriscore_data,nutriments,nutrition_grades for Nutri-Score of 100% Real Orange Juice. The response now contains the Nutri-Score computation: { \"code\" : \"0180411000803\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 11.864406779661 , . . . \"sugars_unit\" : \"g\" , \"sugars_value\" : 11.864406779661 }, \"nutriscore_data\" : { \"energy\" : 195 , \"energy_points\" : 7 , \"energy_value\" : 195 , . . . \"sugars_value\" : 11.86 }, \"nutrition_grades\" : \"c\" , \"product_name\" : \"100% Real Orange Juice\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Add or Edit A Product You can also check the reference cheatsheet to know how to add/edit other types of product data. Search for a Product by Nutri-score # Using the Open Food Facts API, you can filter products based on different criteria. To search for products in the Orange Juice category with a nutrition_grade of c , query the Search for Products endpoint . Describing the Search Request # Make a GET request to the Search for Products endpoint. https://world.openfoodfacts.org/api/v2/search Add the search criteria used to filter the products as query parameters. For Orange Juice with a nutrition_grade of c , add query parameters categories_tags_en to filter Orange Juice while nutrition_grades_tags to filter c . The response will return all the products in the database with the category Orange Juice and nutrition_grade c . https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&nutrition_grades_tags=c To limit the response, add fields to the query parameters to specify the fields to be returned in each product object response. For this tutorial, limit the response to code , product_name , nutrition_grades , and categories_tags_en . https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&nutrition_grades_tags=c&fields=code,nutrition_grades,categories_tags_en The response returns all products that belong to the Orange Juice category, with the nutrition_grade \"c\" and limits each product object response to only the specified fields. It also returns the count(total number) of products that match the search criteria. { \"count\" : 1629 , \"page\" : 1 , \"page_count\" : 24 , \"page_size\" : 24 , \"products\" : [ { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Concentrated fruit juices\" , \"Orange juices\" , \"Concentrated orange juices\" ], \"code\" : \"3123340008288\" , \"nutrition_grades\" : \"c\" }, . . . { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Non-Alcoholic beverages\" , \"Orange juices\" , \"Squeezed juices\" , \"Squeezed orange juices\" ], \"code\" : \"3608580844136\" , \"nutrition_grades\" : \"c\" } ], \"skip\" : 0 } Sorting Search Response # You can proceed to also sort the search response by different fields, for example, sort by the product that was modified last or even by the product_name. Now, let's sort the products with Orange Juice and a nutrition_grade of \"c\" by when they were last modified. To sort the search response, add the sort_by with value last_modified_t as a query parameter to the request. https://world.openfoodfacts.net/api/v2/search?nutrition_grades_tags=c&fields=code,nutrition_grades,categories_tags_en&categories_tags_en=Orange Juice&sort_by=last_modified_t The date that each product was last modified is now used to order the product response. { \"count\" : 1629 , \"page\" : 1 , \"page_count\" : 24 , \"page_size\" : 24 , \"products\" : [ { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Orange juices\" ], \"code\" : \"3800014268048\" , \"nutrition_grades\" : \"c\" }, ' ' ' { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Orange juices\" , \"Squeezed juices\" , \"Squeezed orange juices\" ], \"code\" : \"4056489641018\" , \"nutrition_grades\" : \"c\" } ], \"skip\" : 0 } To see other examples of sorting a search response, see the reference documentation for Search for Products","title":"Using the Open Food Facts API"},{"location":"tutorials/using-the-OFF-API-tutorial/#using-the-open-food-facts-api","text":"","title":"Using the Open Food Facts API"},{"location":"tutorials/using-the-OFF-API-tutorial/#scan-a-product-to-get-nutri-score","text":"This basic tutorial shows you can get the Nutri-score of a product, for instance, to display it in a mobile app after scanning the product barcode. Let's use Nutella Ferrero as the product example for this tutorial. To get a product nutriscore, you need to make a call to the Get A Product By Barcode Endpoint.","title":"Scan A Product To Get Nutri-score"},{"location":"tutorials/using-the-OFF-API-tutorial/#authentication","text":"Usually, no authentication is required to query Get A Product Nutri-score. However, on the staging environment(which is used throughout this tutorial), there is a basic auth to avoid content indexation. For more details, visit the Open Food Facts API Environment .","title":"Authentication"},{"location":"tutorials/using-the-OFF-API-tutorial/#describing-the-get-request","text":"Make a GET request to the Get A Product By Barcode endpoint. https://world.openfoodfacts.net/api/v2/product/{barcode} The {barcode} is the barcode number of the product you are trying to get. The barcode for Nutella Ferrero is 3017624010701 . Then the request path to get product data for Nutella Ferrero will look like this: https://world.openfoodfacts.net/api/v2/product/3017624010701 The response returns every data about Nutella Ferrero on the database. To get the nutriscore, we need to limit the response by specifying the nutriscore field, which is the nutrition_grades and product_name .","title":"Describing the Get Request"},{"location":"tutorials/using-the-OFF-API-tutorial/#query-parameters","text":"To limit the response of the Get A Product By Barcode response, use query parameters to specify the product fields to be returned. In this example, you need one query parameter called field with the value product_name,nutrition_grades . The request path will now look like this: https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data","title":"Query Parameters"},{"location":"tutorials/using-the-OFF-API-tutorial/#nutri-score-response","text":"The response returned contains an object of the code , product , status_verbose , and status . The product object contains the fields specified in the query: the product_name and the nutrition_grades . The status also states if the product was found or not. { \"code\" : \"3017624010701\" , \"product\" : { \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" }","title":"Nutri-Score Response"},{"location":"tutorials/using-the-OFF-API-tutorial/#nutri-score-computation","text":"If you would like to be able to show how the score is computed, add some extra fields like nutriscore_data and nutriments . The request path to get the Nutri-Score computation for Nutella-Ferroro will be : https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data,nutriments,nutrition_grades The product object in the response now contains the extra fields to show how the nutriscore was computed. { \"code\" : \"3017624010701\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 57.5 , \"carbohydrates_100g\" : 57.5 , \"carbohydrates_unit\" : \"g\" , \"carbohydrates_value\" : 57.5 , \"energy\" : 2255 , \"energy-kcal\" : 539 , \"energy-kcal_100g\" : 539 , \"energy-kcal_unit\" : \"kcal\" , ... , ... , \"sugars\" : 56.3 , \"sugars_100g\" : 56.3 , \"sugars_unit\" : \"g\" , \"sugars_value\" : 56.3 }, \"nutriscore_data\" : { \"energy\" : 2255 , \"energy_points\" : 6 , \"energy_value\" : 2255 , ... , ... , \"sugars_points\" : 10 , \"sugars_value\" : 56.3 }, \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Get A Product By Barcode","title":"Nutri-Score Computation"},{"location":"tutorials/using-the-OFF-API-tutorial/#completing-products-to-get-the-nutri-score","text":"","title":"Completing products to get the Nutri-Score"},{"location":"tutorials/using-the-OFF-API-tutorial/#products-without-a-nutri-score","text":"When these fields are missing in a nutriscore computation response, it means that the product does not have a Nutri-Score computation due to some missing nutrition data. Lets look at the 100% Real Orange Juice . If the product nutrition data is missing some fields, you can volunteer and contribute to it by getting the missing tags and writing to the OFF API to add them. To know the missing tags, you need the misc-tags field from the response. https://world.openfoodfacts.net/api/v2/product/0180411000803/100-real-orange-juice?fields=misc_tags The response shows the missing fields and category needed to compute the Nutri-Score. { \"code\" : \"0180411000803\" , \"product\" : { \"misc_tags\" : [ \"en:nutriscore-not-computed\" , \"en:nutriscore-missing-category\" , \"en:nutrition-not-enough-data-to-compute-nutrition-score\" , \"en:nutriscore-missing-nutrition-data\" , \"en:nutriscore-missing-nutrition-data-sodium\" , \"en:ecoscore-extended-data-not-computed\" , \"en:ecoscore-not-computed\" , \"en:main-countries-new-product\" ] }, \"status\" : 1 , \"status_verbose\" : \"product found\" } The sample response above for 100% Real Orange Juice misc_tags shows that the Nutri-Score is missing category ( en:nutriscore-missing-category ) and sodium(salt) ( en:nutriscore-missing-nutrition-data-sodium ). Now you can write to the OFF API to provide these nutriment data (if you have them) so that the Nutri-Score can be computed.","title":"Products without a Nutri-Score"},{"location":"tutorials/using-the-OFF-API-tutorial/#write-data-to-make-nutri-score-computation-possible","text":"The WRITE operations in the OFF API require authentication, therefore you need a valid user_id and password to write the missing nutriment data to 100% Real Orange Juice. Sign up on the Open Food Facts App , to get your user_id and password if you dont have. To write data to a product, make a POST request to the Add or Edit A Product endpoint. https://world.openfoodfacts.net/cgi/product_jqm2.pl For authentication, add your valid user_id and password as body parameters to your request. The code (barcode of the product to be added/edited), user_id and password are required fields when adding or editing a product. Then, include other product data to be added in the request body. To write sodium and category to 100% Real Orange Juice so that the Nutri-Score can be computed, the request body should contain these fields : Key Value Description user_id *** A valid user_id password *** A valid password code 0180411000803 The barcode of the product to be added/edited nutriment_sodium 0.015 Amount of sodium nutriment_sodium_unit g Unit of sodium relative to the amount categories Orange Juice Category of the Product Using curl: curl -XPOST -u off:off -x POST https://world.openfoodfacts.net/cgi/product_jqm2.pl \\ -F user_id = your_user_id -F password = your_password \\ -F code = 0180411000803 -F nutriment_sodium = 0 .015 -F nutriment_sodium_unit = g -F categories = \"Orange Juice\" If the request is succesful, it returns a response that indicated that the fields have been saved. { \"status_verbose\" : \"fields saved\" , \"status\" : 1 }","title":"Write data to make Nutri-Score computation possible"},{"location":"tutorials/using-the-OFF-API-tutorial/#read-newly-computed-nutri-score","text":"Now, let's check if the Nutri-Score for 100% Real Orange Juice has been computed now that we have provided the missing data. Make a GET request to https://world.openfoodfacts.net/api/v2/product/0180411000803?fields=product_name,nutriscore_data,nutriments,nutrition_grades for Nutri-Score of 100% Real Orange Juice. The response now contains the Nutri-Score computation: { \"code\" : \"0180411000803\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 11.864406779661 , . . . \"sugars_unit\" : \"g\" , \"sugars_value\" : 11.864406779661 }, \"nutriscore_data\" : { \"energy\" : 195 , \"energy_points\" : 7 , \"energy_value\" : 195 , . . . \"sugars_value\" : 11.86 }, \"nutrition_grades\" : \"c\" , \"product_name\" : \"100% Real Orange Juice\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Add or Edit A Product You can also check the reference cheatsheet to know how to add/edit other types of product data.","title":"Read newly computed Nutri-Score"},{"location":"tutorials/using-the-OFF-API-tutorial/#search-for-a-product-by-nutri-score","text":"Using the Open Food Facts API, you can filter products based on different criteria. To search for products in the Orange Juice category with a nutrition_grade of c , query the Search for Products endpoint .","title":"Search for a Product by Nutri-score"},{"location":"tutorials/using-the-OFF-API-tutorial/#describing-the-search-request","text":"Make a GET request to the Search for Products endpoint. https://world.openfoodfacts.org/api/v2/search Add the search criteria used to filter the products as query parameters. For Orange Juice with a nutrition_grade of c , add query parameters categories_tags_en to filter Orange Juice while nutrition_grades_tags to filter c . The response will return all the products in the database with the category Orange Juice and nutrition_grade c . https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&nutrition_grades_tags=c To limit the response, add fields to the query parameters to specify the fields to be returned in each product object response. For this tutorial, limit the response to code , product_name , nutrition_grades , and categories_tags_en . https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&nutrition_grades_tags=c&fields=code,nutrition_grades,categories_tags_en The response returns all products that belong to the Orange Juice category, with the nutrition_grade \"c\" and limits each product object response to only the specified fields. It also returns the count(total number) of products that match the search criteria. { \"count\" : 1629 , \"page\" : 1 , \"page_count\" : 24 , \"page_size\" : 24 , \"products\" : [ { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Concentrated fruit juices\" , \"Orange juices\" , \"Concentrated orange juices\" ], \"code\" : \"3123340008288\" , \"nutrition_grades\" : \"c\" }, . . . { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Non-Alcoholic beverages\" , \"Orange juices\" , \"Squeezed juices\" , \"Squeezed orange juices\" ], \"code\" : \"3608580844136\" , \"nutrition_grades\" : \"c\" } ], \"skip\" : 0 }","title":"Describing the Search Request"},{"location":"tutorials/using-the-OFF-API-tutorial/#sorting-search-response","text":"You can proceed to also sort the search response by different fields, for example, sort by the product that was modified last or even by the product_name. Now, let's sort the products with Orange Juice and a nutrition_grade of \"c\" by when they were last modified. To sort the search response, add the sort_by with value last_modified_t as a query parameter to the request. https://world.openfoodfacts.net/api/v2/search?nutrition_grades_tags=c&fields=code,nutrition_grades,categories_tags_en&categories_tags_en=Orange Juice&sort_by=last_modified_t The date that each product was last modified is now used to order the product response. { \"count\" : 1629 , \"page\" : 1 , \"page_count\" : 24 , \"page_size\" : 24 , \"products\" : [ { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Orange juices\" ], \"code\" : \"3800014268048\" , \"nutrition_grades\" : \"c\" }, ' ' ' { \"categories_tags_en\" : [ \"Plant-based foods and beverages\" , \"Beverages\" , \"Plant-based beverages\" , \"Fruit-based beverages\" , \"Juices and nectars\" , \"Fruit juices\" , \"Orange juices\" , \"Squeezed juices\" , \"Squeezed orange juices\" ], \"code\" : \"4056489641018\" , \"nutrition_grades\" : \"c\" } ], \"skip\" : 0 } To see other examples of sorting a search response, see the reference documentation for Search for Products","title":"Sorting Search Response"}]}